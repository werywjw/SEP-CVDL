{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from model import GiMeFive\n",
    "from pytorch_grad_cam import GradCAM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import dlib\n",
    "import argparse\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple hook class that returns the input and output of a layer during forward and backward pass\n",
    "class Hook():\n",
    "    def __init__(self):\n",
    "        self.hook_forward = None\n",
    "        self.hook_backward = None\n",
    "        self.forward_out = None\n",
    "        self.backward_out = None\n",
    "\n",
    "    def hook_fn_forward(self, module, input, output):\n",
    "        self.forward_out = output\n",
    "\n",
    "    def hook_fn_backward(self, module, grad_input, grad_output):\n",
    "        self.backward_out = grad_output[0] \n",
    "\n",
    "    def register_hook(self, module):\n",
    "        self.hook_forward = module.register_forward_hook(self.hook_fn_forward)\n",
    "        self.hook_backward = module.register_full_backward_hook(self.hook_fn_backward)\n",
    "\n",
    "    def unregister_hook(self):\n",
    "        self.hook_forward.remove()\n",
    "        self.hook_backward.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class_labels = ['happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear']\n",
    "\n",
    "model = GiMeFive().to(device)\n",
    "model.load_state_dict(torch.load('best_GiMeFive.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "final_layer = model.conv5\n",
    "hook = Hook()\n",
    "hook.register_hook(final_layer)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def classify_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    image_array = np.array(image)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "    scores = probabilities.cpu().numpy().flatten()\n",
    "    rounded_scores = [round(score, 2) for score in scores]\n",
    "    \n",
    "    return rounded_scores, image, image_array, image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV Real-Time Face Detection, dlib Landmarks, saving camera / video\n",
    "## class_labels = ['happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear']\n",
    "\n",
    "from pathlib import Path\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# text settings\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 1\n",
    "font_color = (154, 1, 254) # BGR color neon pink 254,1,154\n",
    "thickness = 2\n",
    "line_type = cv2.LINE_AA\n",
    "\n",
    "max_emotion = ''\n",
    "transparency = 0.4\n",
    "\n",
    "def detect_emotion(pil_crop_img):\n",
    "    # Convert NumPy array to PIL Image\n",
    "    pil_crop_img = Image.fromarray(pil_crop_img)\n",
    "    \n",
    "    vid_fr_tensor = transform(pil_crop_img).unsqueeze(0).to(device)\n",
    "    # with torch.no_grad():\n",
    "    logits = model(vid_fr_tensor)\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "    predicted_class_idx = predicted_class.item()\n",
    "\n",
    "    one_hot_output = torch.FloatTensor(1, probabilities.shape[1]).zero_()\n",
    "    one_hot_output[0][predicted_class_idx] = 1\n",
    "    logits.backward(one_hot_output, retain_graph=True)\n",
    "\n",
    "    gradients = hook.backward_out\n",
    "    feature_maps = hook.forward_out\n",
    "\n",
    "    weights = torch.mean(gradients, dim=[2, 3], keepdim=True)\n",
    "    cam = torch.sum(weights * feature_maps, dim=1, keepdim=True)\n",
    "    cam = cam.clamp(min=0).squeeze() \n",
    "\n",
    "    cam -= cam.min()\n",
    "    cam /= cam.max()\n",
    "    cam = cam.cpu().detach().numpy()\n",
    "\n",
    "    # scores = probabilities.cpu().numpy().flatten()\n",
    "    scores = probabilities.cpu().detach().numpy().flatten()\n",
    "    rounded_scores = [round(score, 2) for score in scores]\n",
    "    return rounded_scores, cam\n",
    "\n",
    "def plot_heatmap(x, y, w, h, cam, pil_crop_img, video_frame):\n",
    "    # resize cam to w, h\n",
    "    cam = cv2.resize(cam, (w, h))\n",
    "    \n",
    "    # apply color map to resized cam\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    \n",
    "    # Get the region of interest on the video frame\n",
    "    roi = video_frame[y:y+h, x:x+w, :]\n",
    "\n",
    "    # Blend the heatmap with the ROI\n",
    "    overlay = heatmap * transparency + roi / 255 * (1 - transparency)\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "\n",
    "    # Replace the ROI with the blended overlay\n",
    "    video_frame[y:y+h, x:x+w, :] = np.uint8(255 * overlay)\n",
    "        \n",
    "def update_max_emotion(rounded_scores):  \n",
    "    # get index from max value in rounded_scores\n",
    "    max_index = np.argmax(rounded_scores)\n",
    "    max_emotion = class_labels[max_index]\n",
    "    return max_emotion # returns max_emotion as string\n",
    "\n",
    "def print_max_emotion(x, y, max_emotion, video_frame):\n",
    "    # position to put the text for the max emotion\n",
    "    org = (x, y - 15)\n",
    "    cv2.putText(video_frame, max_emotion, org, font, font_scale, font_color, thickness, line_type)\n",
    "    \n",
    "def print_all_emotion(x, y, w, rounded_scores, video_frame):\n",
    "    # create text to be displayed\n",
    "    org = (x + w + 10, y - 20)\n",
    "    for index, value in enumerate(class_labels):\n",
    "        emotion_str = (f'{value}: {rounded_scores[index]:.2f}')\n",
    "        y = org[1] + 40\n",
    "        org = (org[0], y)\n",
    "        cv2.putText(video_frame, emotion_str, org, font, font_scale, font_color, thickness, line_type)\n",
    "    \n",
    "# identify Face in Video Stream\n",
    "def detect_bounding_box(video_frame, counter):\n",
    "    global max_emotion\n",
    "    gray_image = cv2.cvtColor(video_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # notes: MultiScale optimized\n",
    "    faces = face_classifier.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=10, minSize=(64, 64))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(gray_image, (x, y), (x+w, y+h), (255, 0, 0), 0)\n",
    "\n",
    "        # convert the ROI to a dlib rectangle\n",
    "        dlib_rect = dlib.rectangle(x, y, x+w, y+h)\n",
    "\n",
    "        # detect facial landmarks through dlib\n",
    "        landmarks = predictor(gray_image, dlib_rect)\n",
    "\n",
    "        pil_crop_img = video_frame[y : y + h, x : x + w]\n",
    "        rounded_scores, cam = detect_emotion(pil_crop_img)\n",
    "            \n",
    "        if counter == 0:\n",
    "            max_emotion = update_max_emotion(rounded_scores) \n",
    "            \n",
    "        # draw landmarks on the video_frame\n",
    "        for i in range(68):  # Assuming you have 68 landmarks\n",
    "            cv2.circle(video_frame, (landmarks.part(i).x, landmarks.part(i).y), 1, (255, 255, 255), 0)\n",
    "            \n",
    "        plot_heatmap(x, y, w, h, cam, pil_crop_img, video_frame)\n",
    "        print_max_emotion(x, y, max_emotion, video_frame) # displays the max_emotion according to evaluation_frequency\n",
    "        print_all_emotion(x, y, w, rounded_scores, video_frame) # evaluates every video_frame for debugging\n",
    "\n",
    "    return faces\n",
    "\n",
    "def create_video_out(source, input_path_to_video):\n",
    "    if source == 'camera':\n",
    "        video_capture = cv2.VideoCapture(0)\n",
    "        fps = 10\n",
    "        out_file_name = 'cam_eval_video.mp4'\n",
    "    elif source == 'video':\n",
    "        video_capture = cv2.VideoCapture(input_path_to_video)\n",
    "        fps = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
    "        out_file_name = 'eval_video.mp4'\n",
    "    else:\n",
    "        print('unknown input')\n",
    "        print('please enter camera or video')\n",
    "    frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(out_file_name, fourcc, fps, (frame_width, frame_height))\n",
    "    return out, video_capture\n",
    "\n",
    "\n",
    "# loop for Real-Time Face Detection\n",
    "def evaluate_input(source, input_path_to_video):\n",
    "    out, video_capture = create_video_out(source, input_path_to_video)\n",
    "    \n",
    "    counter = 0\n",
    "    evaluation_frequency = 5\n",
    "\n",
    "    while True:\n",
    "\n",
    "        result, video_frame = video_capture.read()  # read frames from the video\n",
    "        if result is False:\n",
    "            break  # terminate the loop if the frame is not read successfully\n",
    "        \n",
    "        faces = detect_bounding_box(video_frame, counter)  # apply the function we created to the video frame, faces as variable not used\n",
    "        \n",
    "        cv2.imshow(\"My Face Detection Project\", video_frame)  # display the processed frame in a window named \"My Face Detection Project\"\n",
    "\n",
    "        out.write(video_frame)  # write the processed frame to the output video file\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "        \n",
    "        counter += 1\n",
    "        if counter == evaluation_frequency:\n",
    "            counter = 0\n",
    "\n",
    "    hook.unregister_hook()        \n",
    "    video_capture.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "/var/folders/rj/_c5q74xj60x1t6st889f_y900000gn/T/ipykernel_14045/3022206585.py:57: RuntimeWarning: invalid value encountered in cast\n",
      "  heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n"
     ]
    }
   ],
   "source": [
    "test_source = 'video'\n",
    "test_input_path_to_video = 'video/test_video_noemotions.mp4'\n",
    "evaluate_input(test_source, test_input_path_to_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(args):\n",
    "#     print(args)\n",
    "#     evaluate_input(args.source, args.input_path_to_video)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(\n",
    "#         formatter_class=argparse.RawTextHelpFormatter,\n",
    "#         description=textwrap.dedent(\n",
    "#             '''\\\n",
    "#             This program performs emotion evaluation of faces from given video or camera feed.\n",
    "            \n",
    "#             If \"camera\" is selected it will generate a video with analyzed emotions from live stream,\n",
    "#             and store it in the file cam_eval_video.mp4.\n",
    "            \n",
    "#             If \"video\" is selected it will analyze emotions from given video,\n",
    "#             and store it in the file eval_video.mp4.\n",
    "#             ''')\n",
    "#     )\n",
    "#     parser.add_argument('-s', '--source', type=str, help='Enter \"camera\" or \"video\"', default='video')\n",
    "#     parser.add_argument('-i', '--input_path_to_video', type=str, help='Path to the video file.', default='test_video/test_video_noemotions02.mp4')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
