{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from model import EmotionClassifier\n",
    "from pytorch_grad_cam import GradCAM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import dlib\n",
    "import argparse\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple hook class that returns the input and output of a layer during forward and backward pass\n",
    "class Hook():\n",
    "    def __init__(self):\n",
    "        self.hook_forward = None\n",
    "        self.hook_backward = None\n",
    "        self.forward_out = None\n",
    "        self.backward_out = None\n",
    "\n",
    "    def hook_fn_forward(self, module, input, output):\n",
    "        self.forward_out = output\n",
    "\n",
    "    def hook_fn_backward(self, module, grad_input, grad_output):\n",
    "        self.backward_out = grad_output[0] \n",
    "\n",
    "    def register_hook(self, module):\n",
    "        self.hook_forward = module.register_forward_hook(self.hook_fn_forward)\n",
    "        self.hook_backward = module.register_full_backward_hook(self.hook_fn_backward)\n",
    "\n",
    "    def unregister_hook(self):\n",
    "        self.hook_forward.remove()\n",
    "        self.hook_backward.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class_labels = ['happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear']\n",
    "\n",
    "model = EmotionClassifier().to(device)\n",
    "model.load_state_dict(torch.load('best_RAF.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "final_layer = model.conv5\n",
    "hook = Hook()\n",
    "hook.register_hook(final_layer)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def classify_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    image_array = np.array(image)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "    scores = probabilities.cpu().numpy().flatten()\n",
    "    rounded_scores = [round(score, 2) for score in scores]\n",
    "    \n",
    "    return rounded_scores, image, image_array, image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV Real-Time Face Detection, dlib Landmarks, saving camera / video\n",
    "## class_labels = ['happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear']\n",
    "\n",
    "from pathlib import Path\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# face cascade\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# text settings\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 1\n",
    "font_color = (154, 1, 254) # BGR color neon pink 254,1,154\n",
    "thickness = 2\n",
    "line_type = cv2.LINE_AA\n",
    "\n",
    "max_emotion = ''\n",
    "transparency = 0.4\n",
    "\n",
    "def detect_emotion(pil_crop_img):\n",
    "    # Convert NumPy array to PIL Image\n",
    "    pil_crop_img = Image.fromarray(pil_crop_img)\n",
    "    \n",
    "    vid_fr_tensor = transform(pil_crop_img).unsqueeze(0).to(device)\n",
    "    # with torch.no_grad():\n",
    "    logits = model(vid_fr_tensor)\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "    predicted_class_idx = predicted_class.item()\n",
    "\n",
    "    one_hot_output = torch.FloatTensor(1, probabilities.shape[1]).zero_()\n",
    "    one_hot_output[0][predicted_class_idx] = 1\n",
    "    logits.backward(one_hot_output, retain_graph=True)\n",
    "\n",
    "    gradients = hook.backward_out\n",
    "    feature_maps = hook.forward_out\n",
    "\n",
    "    weights = torch.mean(gradients, dim=[2, 3], keepdim=True)\n",
    "    cam = torch.sum(weights * feature_maps, dim=1, keepdim=True)\n",
    "    cam = cam.clamp(min=0).squeeze() \n",
    "\n",
    "    cam -= cam.min()\n",
    "    cam /= cam.max()\n",
    "    cam = cam.cpu().detach().numpy()\n",
    "\n",
    "    # scores = probabilities.cpu().numpy().flatten()\n",
    "    scores = probabilities.cpu().detach().numpy().flatten()\n",
    "    rounded_scores = [round(score, 2) for score in scores]\n",
    "    return rounded_scores, cam\n",
    "\n",
    "def plot_heatmap(x, y, w, h, cam, pil_crop_img, video_frame):\n",
    "    # resize cam to w, h\n",
    "    cam = cv2.resize(cam, (w, h))\n",
    "    \n",
    "    # apply color map to resized cam\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    \n",
    "    # Get the region of interest on the video frame\n",
    "    roi = video_frame[y:y+h, x:x+w, :]\n",
    "\n",
    "    # Blend the heatmap with the ROI\n",
    "    overlay = heatmap * transparency + roi / 255 * (1 - transparency)\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "\n",
    "    # Replace the ROI with the blended overlay\n",
    "    video_frame[y:y+h, x:x+w, :] = np.uint8(255 * overlay)\n",
    "        \n",
    "def update_max_emotion(rounded_scores):  \n",
    "    # get index from max value in rounded_scores\n",
    "    max_index = np.argmax(rounded_scores)\n",
    "    max_emotion = class_labels[max_index]\n",
    "    return max_emotion # returns max_emotion as string\n",
    "\n",
    "def print_max_emotion(x, y, max_emotion, video_frame):\n",
    "    # position to put the text for the max emotion\n",
    "    org = (x, y - 15)\n",
    "    cv2.putText(video_frame, max_emotion, org, font, font_scale, font_color, thickness, line_type)\n",
    "    \n",
    "def print_all_emotion(x, y, w, rounded_scores, video_frame):\n",
    "    # create text to be displayed\n",
    "    org = (x + w + 10, y - 20)\n",
    "    for index, value in enumerate(class_labels):\n",
    "        emotion_str = (f'{value}: {rounded_scores[index]:.2f}')\n",
    "        y = org[1] + 40\n",
    "        org = (org[0], y)\n",
    "        cv2.putText(video_frame, emotion_str, org, font, font_scale, font_color, thickness, line_type)\n",
    "    \n",
    "# identify Face in Video Stream\n",
    "def detect_bounding_box(video_frame, counter):\n",
    "    global max_emotion\n",
    "    gray_image = cv2.cvtColor(video_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # notes: MultiScale optimized\n",
    "    faces = face_classifier.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=10, minSize=(64, 64))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(gray_image, (x, y), (x+w, y+h), (255, 0, 0), 0)\n",
    "\n",
    "        # convert the ROI to a dlib rectangle\n",
    "        dlib_rect = dlib.rectangle(x, y, x+w, y+h)\n",
    "\n",
    "        # detect facial landmarks through dlib\n",
    "        landmarks = predictor(gray_image, dlib_rect)\n",
    "\n",
    "        pil_crop_img = video_frame[y : y + h, x : x + w]\n",
    "        rounded_scores, cam = detect_emotion(pil_crop_img)\n",
    "            \n",
    "        if counter == 0:\n",
    "            max_emotion = update_max_emotion(rounded_scores) \n",
    "            \n",
    "        # draw landmarks on the video_frame\n",
    "        for i in range(68):  # Assuming you have 68 landmarks\n",
    "            cv2.circle(video_frame, (landmarks.part(i).x, landmarks.part(i).y), 1, (255, 255, 255), 0)\n",
    "            \n",
    "        plot_heatmap(x, y, w, h, cam, pil_crop_img, video_frame)\n",
    "        print_max_emotion(x, y, max_emotion, video_frame) # displays the max_emotion according to evaluation_frequency\n",
    "        print_all_emotion(x, y, w, rounded_scores, video_frame) # evaluates every video_frame for debugging\n",
    "\n",
    "    return faces\n",
    "\n",
    "def create_video_out(source, input_path_to_video):\n",
    "    if source == 'camera':\n",
    "        video_capture = cv2.VideoCapture(0)\n",
    "        fps = 10\n",
    "        out_file_name = 'cam_eval_video.mp4'\n",
    "    elif source == 'video':\n",
    "        video_capture = cv2.VideoCapture(input_path_to_video)\n",
    "        fps = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
    "        out_file_name = 'eval_video.mp4'\n",
    "    else:\n",
    "        print('unknown input')\n",
    "        print('please enter camera or video')\n",
    "    frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(out_file_name, fourcc, fps, (frame_width, frame_height))\n",
    "    return out, video_capture\n",
    "\n",
    "\n",
    "# loop for Real-Time Face Detection\n",
    "def evaluate_input(source, input_path_to_video):\n",
    "    out, video_capture = create_video_out(source, input_path_to_video)\n",
    "    \n",
    "    counter = 0\n",
    "    evaluation_frequency = 5\n",
    "\n",
    "    while True:\n",
    "\n",
    "        result, video_frame = video_capture.read()  # read frames from the video\n",
    "        if result is False:\n",
    "            break  # terminate the loop if the frame is not read successfully\n",
    "        \n",
    "        faces = detect_bounding_box(video_frame, counter)  # apply the function we created to the video frame, faces as variable not used\n",
    "        \n",
    "        cv2.imshow(\"My Face Detection Project\", video_frame)  # display the processed frame in a window named \"My Face Detection Project\"\n",
    "\n",
    "        out.write(video_frame)  # write the processed frame to the output video file\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "        \n",
    "        counter += 1\n",
    "        if counter == evaluation_frequency:\n",
    "            counter = 0\n",
    "\n",
    "    hook.unregister_hook()        \n",
    "    video_capture.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "/var/folders/rj/_c5q74xj60x1t6st889f_y900000gn/T/ipykernel_91573/993582917.py:62: RuntimeWarning: invalid value encountered in cast\n",
      "  heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m test_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m test_input_path_to_video \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_video/test_video_noemotions02.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mevaluate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_input_path_to_video\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 190\u001b[0m, in \u001b[0;36mevaluate_input\u001b[0;34m(source, input_path_to_video)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# terminate the loop if the frame is not read successfully\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_bounding_box\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounter\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# apply the function we created to the video frame, faces as variable not used\u001b[39;00m\n\u001b[1;32m    192\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy Face Detection Project\u001b[39m\u001b[38;5;124m\"\u001b[39m, video_frame)  \u001b[38;5;66;03m# display the processed frame in a window named \"My Face Detection Project\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m out\u001b[38;5;241m.\u001b[39mwrite(video_frame)  \u001b[38;5;66;03m# write the processed frame to the output video file\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 117\u001b[0m, in \u001b[0;36mdetect_bounding_box\u001b[0;34m(video_frame, counter)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# check if there are enough landmarks\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# if len(landmarks.parts()) >= landmark_threshhold:\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# draw bounding box on face\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# cv2.rectangle(gray_image, (x, y), (x+w, y+h), (255, 0, 0), 0)\u001b[39;00m\n\u001b[1;32m    116\u001b[0m pil_crop_img \u001b[38;5;241m=\u001b[39m video_frame[y : y \u001b[38;5;241m+\u001b[39m h, x : x \u001b[38;5;241m+\u001b[39m w]\n\u001b[0;32m--> 117\u001b[0m rounded_scores, cam \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_emotion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpil_crop_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m counter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    120\u001b[0m     max_emotion \u001b[38;5;241m=\u001b[39m update_max_emotion(rounded_scores) \n",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m, in \u001b[0;36mdetect_emotion\u001b[0;34m(pil_crop_img)\u001b[0m\n\u001b[1;32m     28\u001b[0m vid_fr_tensor \u001b[38;5;241m=\u001b[39m transform(pil_crop_img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvid_fr_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(probabilities, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/SEP-CVDL/model.py:75\u001b[0m, in \u001b[0;36mEmotionClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(x, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     76\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(x, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     77\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn4(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4(x)))\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test_source = 'video'\n",
    "# test_input_path_to_video = 'test_video/test_video_noemotions02.mp4'\n",
    "# evaluate_input(test_source, test_input_path_to_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     evaluate_input(args\u001b[38;5;241m.\u001b[39msource, args\u001b[38;5;241m.\u001b[39minput_path_to_video)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 6\u001b[0m     parser \u001b[38;5;241m=\u001b[39m \u001b[43margparse\u001b[49m\u001b[38;5;241m.\u001b[39mArgumentParser(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m                                     This program performs emotion evaluation of faces from given video or camera feed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m                                     \u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m                                     If \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcamera\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is selected it will generate a video with analyzed emotions from live stream,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m                                     and store it in the file cam_eval_video.mp4.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m                                     If \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is selected it will analyze emotions from given video,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m                                     and store it in the file eval_video.mp4.\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m                                     \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-s\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--source\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnter \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcamera\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m     parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-i\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--input_path_to_video\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPath to the video file.\u001b[39m\u001b[38;5;124m'\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_video/test_video_noemotions02.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    print(args)\n",
    "    evaluate_input(args.source, args.input_path_to_video)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.RawTextHelpFormatter,\n",
    "        description=textwrap.dedent(\n",
    "            '''\\\n",
    "            This program performs emotion evaluation of faces from given video or camera feed.\n",
    "            \n",
    "            If \"camera\" is selected it will generate a video with analyzed emotions from live stream,\n",
    "            and store it in the file cam_eval_video.mp4.\n",
    "            If \"video\" is selected it will analyze emotions from given video,\n",
    "            and store it in the file eval_video.mp4.\n",
    "            ''')\n",
    "    )\n",
    "    parser.add_argument('-s', '--source', type=str, help='Enter \"camera\" or \"video\"', default='video')\n",
    "    parser.add_argument('-i', '--input_path_to_video', type=str, help='Path to the video file.', default='test_video/test_video_noemotions02.mp4')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
