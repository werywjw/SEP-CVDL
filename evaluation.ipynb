{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, Sequential, Module\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from model import EmotionClassifier\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import imageio\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hook():\n",
    "    ''' \n",
    "    A simple hook class that returns the input and output of a layer during forward and backward pass\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.hook_forward = None\n",
    "        self.hook_backward = None\n",
    "        self.forward_out = None\n",
    "        self.backward_out = None\n",
    "\n",
    "    def hook_fn_forward(self, module, input, output):\n",
    "        self.forward_out = output\n",
    "\n",
    "    def hook_fn_backward(self, module, grad_input, grad_output):\n",
    "        self.backward_out = grad_output[0] \n",
    "\n",
    "    def register_hook(self, module):\n",
    "        self.hook_forward = module.register_forward_hook(self.hook_fn_forward)\n",
    "        self.hook_backward = module.register_full_backward_hook(self.hook_fn_backward)\n",
    "\n",
    "    def unregister_hook(self):\n",
    "        self.hook_forward.remove()\n",
    "        self.hook_backward.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class_labels = ['happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear']\n",
    "\n",
    "model = EmotionClassifier().to(device)\n",
    "model.load_state_dict(torch.load('best_RAF.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "final_layer = model.conv5\n",
    "hook = Hook()\n",
    "hook.register_hook(final_layer)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def classify_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    image_array = np.array(image)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "    scores = probabilities.cpu().numpy().flatten()\n",
    "    rounded_scores = [round(score, 2) for score in scores]\n",
    "    return rounded_scores, image, image_array, image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cascade Landmarks, reading images / videos\n",
    "\n",
    "# # OpenCV Real-Time Face Detection\n",
    "# ## class_labels = ['happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear']\n",
    "\n",
    "# face_classifier = cv2.CascadeClassifier(\n",
    "#     cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# # access webcam as numpy.ndarray\n",
    "# video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# # text settings\n",
    "# font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "# font_scale = 1\n",
    "# font_color = (0, 255, 0)  # BGR color\n",
    "# thickness = 2\n",
    "# line_type = cv2.LINE_AA\n",
    "\n",
    "# max_emotion = ''\n",
    "\n",
    "# def detect_emotion(video_frame):\n",
    "#     vid_fr_tensor = transform(video_frame).unsqueeze(0).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(vid_fr_tensor)\n",
    "#         probabilities = F.softmax(outputs, dim=1)\n",
    "#     scores = probabilities.cpu().numpy().flatten()\n",
    "#     rounded_scores = [round(score, 2) for score in scores]\n",
    "#     # print(f'rounded_scores in detect_emotion {rounded_scores}')\n",
    "#     return rounded_scores\n",
    "\n",
    "# def get_max_emotion(x, y, w, h, video_frame):\n",
    "#     crop_img = video_frame[y : y + h, x : x + w]\n",
    "#     pil_crop_img = Image.fromarray(crop_img)\n",
    "#     # slower cropping\n",
    "#     rounded_scores = detect_emotion(pil_crop_img)    \n",
    "#     # get index from max value in rounded_scores\n",
    "#     max_index = np.argmax(rounded_scores)\n",
    "#     max_emotion = class_labels[max_index]\n",
    "#     # print(f'max_emotion: {max_emotion}')\n",
    "\n",
    "#     return max_emotion\n",
    "\n",
    "# def print_max_emotion(x, y, video_frame, max_emotion):\n",
    "#     # position to put the text for the max emotion\n",
    "#     org = (x, y - 15)\n",
    "#     cv2.putText(video_frame, max_emotion, org, font, font_scale, font_color, thickness, line_type)\n",
    "    \n",
    "# def print_all_emotion(x, y, w, h, video_frame):\n",
    "#     crop_img = video_frame[y : y + h, x : x + w]\n",
    "#     pil_crop_img = Image.fromarray(crop_img)\n",
    "#     # slower cropping\n",
    "#     rounded_scores = detect_emotion(pil_crop_img)\n",
    "#     # print(f'rounded_scores in detect_bounding_box: {rounded_scores}')\n",
    "#     # create text to be displayed\n",
    "#     org = (x + w + 10, y - 20)\n",
    "#     for index, value in enumerate(class_labels):\n",
    "#         emotion_str = (f'{value}: {rounded_scores[index]:.2f}')\n",
    "#         y = org[1] + 40\n",
    "#         org = (org[0], y)\n",
    "#         cv2.putText(video_frame, emotion_str, org, font, font_scale, font_color, thickness, line_type)\n",
    "    \n",
    "# # identify Face in Video Stream\n",
    "# def detect_bounding_box(video_frame, counter):\n",
    "#     global max_emotion\n",
    "#     gray_image = cv2.cvtColor(video_frame, cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         # draw bounding box on face\n",
    "#         cv2.rectangle(video_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "#         # crop bounding box\n",
    "#         if counter == 0:\n",
    "#             max_emotion = get_max_emotion(x, y, w, h, video_frame) \n",
    "        \n",
    "#         print_max_emotion(x, y, video_frame, max_emotion) # displays the max_emotion according to evaluation_frequency\n",
    "#         print_all_emotion(x, y, w, h, video_frame) # evaluates every video_frame for debugging\n",
    "\n",
    "#     return faces\n",
    "\n",
    "# counter = 0\n",
    "# evaluation_frequency = 5\n",
    "\n",
    "# # Loop for Real-Time Face Detection\n",
    "# while True:\n",
    "\n",
    "#     result, video_frame = video_capture.read()  # read frames from the video\n",
    "#     if result is False:\n",
    "#         break  # terminate the loop if the frame is not read successfully\n",
    "    \n",
    "#     faces = detect_bounding_box(video_frame, counter)  # apply the function we created to the video frame, faces as variable not used\n",
    "    \n",
    "#     cv2.imshow(\"My Face Detection Project\", video_frame)  # display the processed frame in a window named \"My Face Detection Project\"\n",
    "\n",
    "#     # print(type(video_frame))\n",
    "#     if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "#         break\n",
    "    \n",
    "#     counter += 1\n",
    "#     if counter == evaluation_frequency:\n",
    "#         counter = 0\n",
    "        \n",
    "# video_capture.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from : lbfmodel.yaml\n",
      "landmark_detector < cv2.face.Facemark 0x111a737d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rj/_c5q74xj60x1t6st889f_y900000gn/T/ipykernel_98750/930467398.py:88: RuntimeWarning: invalid value encountered in cast\n",
      "  heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n"
     ]
    }
   ],
   "source": [
    "# LBF Landmarks, saving video\n",
    "\n",
    "# OpenCV Real-Time Face Detection\n",
    "## class_labels = ['happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear']\n",
    "\n",
    "import dlib\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# save facial landmark detection model's name as LBFmodel\n",
    "LBFmodel = \"lbfmodel.yaml\"\n",
    "\n",
    "# create an instance of the Facial landmark Detector with the model\n",
    "landmark_detector  = cv2.face.createFacemarkLBF()\n",
    "landmark_detector.loadModel(LBFmodel)\n",
    "print(f'landmark_detector {landmark_detector}')\n",
    "\n",
    "# text settings\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 1\n",
    "# font_color = (255, 217, 4) # BGR color neon blue\n",
    "# font_color = (96, 252, 253) # BGR color neon yellow\n",
    "font_color = (154, 1, 254) # BGR color neon pink 254,1,154\n",
    "thickness = 2\n",
    "line_type = cv2.LINE_AA\n",
    "\n",
    "\n",
    "max_emotion = ''\n",
    "transparency = 0.4\n",
    "landmark_threshhold = 65\n",
    "\n",
    "def lbf_check(gray_image, faces):\n",
    "    # detect landmarks on \"image_gray\"\n",
    "    _, landmarks = landmark_detector.fit(gray_image, faces)\n",
    "#     # print(type(landmarks))\n",
    "#     # print(landmarks)\n",
    "#     # print(landmarks[0])\n",
    "#     # print(type(landmarks[0]))\n",
    "#     # print(landmarks[0].shape)\n",
    "#     # print(f'landmarks in lbf_check {len(landmarks)}')\n",
    "#     # print(f'landmarks in lbf_check {landmarks}')\n",
    "#     # print(f'Dimentsions:', landmarks.shape)\n",
    "#     # for landmark in landmarks:\n",
    "#     #     for x,y in landmark[0]:\n",
    "#     #         # display landmarks on \"image_cropped\"\n",
    "#     #         # with white colour in BGR and thickness 1\n",
    "#     #         cv2.circle(video_frame, (x + face[0], y + face[1]), 1, (255, 255, 255), 1)\n",
    "#     #         # cv2.circle(image_cropped, ((int)x, (int)y), 1, (255, 255, 255), 1)\n",
    "#     number_of_landmarks = landmarks[0].shape[1]\n",
    "#     # print(f'number_of_landmarks {number_of_landmarks}')\n",
    "#     return number_of_landmarks\n",
    "\n",
    "def detect_emotion(pil_crop_img):\n",
    "    vid_fr_tensor = transform(pil_crop_img).unsqueeze(0).to(device)\n",
    "    # with torch.no_grad():\n",
    "    logits = model(vid_fr_tensor)\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "    predicted_class_idx = predicted_class.item()\n",
    "\n",
    "    one_hot_output = torch.FloatTensor(1, probabilities.shape[1]).zero_()\n",
    "    one_hot_output[0][predicted_class_idx] = 1\n",
    "    logits.backward(one_hot_output, retain_graph=True)\n",
    "\n",
    "    gradients = hook.backward_out\n",
    "    feature_maps = hook.forward_out\n",
    "\n",
    "    weights = torch.mean(gradients, dim=[2, 3], keepdim=True)\n",
    "    cam = torch.sum(weights * feature_maps, dim=1, keepdim=True)\n",
    "    cam = cam.clamp(min=0).squeeze() \n",
    "\n",
    "    cam -= cam.min()\n",
    "    cam /= cam.max()\n",
    "    cam = cam.cpu().detach().numpy()\n",
    "\n",
    "    # scores = probabilities.cpu().numpy().flatten()\n",
    "    scores = probabilities.cpu().detach().numpy().flatten()\n",
    "    rounded_scores = [round(score, 2) for score in scores]\n",
    "    # print(f'rounded_scores in detect_emotion {rounded_scores}')\n",
    "    return rounded_scores, cam\n",
    "\n",
    "def plot_heatmap(x, y, w, h, cam, pil_crop_img, video_frame):\n",
    "    # resize cam to w, h\n",
    "    cam = cv2.resize(cam, (w, h))\n",
    "    \n",
    "    # apply color map to resized cam\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    \n",
    "    # Get the region of interest on the video frame\n",
    "    roi = video_frame[y:y+h, x:x+w, :]\n",
    "\n",
    "    # Blend the heatmap with the ROI\n",
    "    overlay = heatmap * transparency + roi / 255 * (1 - transparency)\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "\n",
    "    # Replace the ROI with the blended overlay\n",
    "    video_frame[y:y+h, x:x+w, :] = np.uint8(255 * overlay)\n",
    "        \n",
    "def update_max_emotion(rounded_scores):  \n",
    "    # get index from max value in rounded_scores\n",
    "    max_index = np.argmax(rounded_scores)\n",
    "    max_emotion = class_labels[max_index]\n",
    "    return max_emotion # returns max_emotion as string\n",
    "\n",
    "def print_max_emotion(x, y, max_emotion, video_frame):\n",
    "    # position to put the text for the max emotion\n",
    "    org = (x, y - 15)\n",
    "    cv2.putText(video_frame, max_emotion, org, font, font_scale, font_color, thickness, line_type)\n",
    "    \n",
    "def print_all_emotion(x, y, w, rounded_scores, video_frame):\n",
    "    # create text to be displayed\n",
    "    org = (x + w + 10, y - 20)\n",
    "    for index, value in enumerate(class_labels):\n",
    "        emotion_str = (f'{value}: {rounded_scores[index]:.2f}')\n",
    "        y = org[1] + 40\n",
    "        org = (org[0], y)\n",
    "        cv2.putText(video_frame, emotion_str, org, font, font_scale, font_color, thickness, line_type)\n",
    "    \n",
    "# identify Face in Video Stream\n",
    "def detect_bounding_box(video_frame, counter):\n",
    "    global max_emotion\n",
    "    gray_image = cv2.cvtColor(video_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = np.array([[x, y, w, h]])  # Convert (x, y, w, h) to a NumPy array\n",
    "        # if lbf_check(gray_image, roi) > landmark_threshhold:\n",
    "        crop_img = video_frame[y : y + h, x : x + w]\n",
    "        pil_crop_img = Image.fromarray(crop_img)\n",
    "        rounded_scores, cam = detect_emotion(pil_crop_img)  \n",
    "        \n",
    "        if counter == 0:\n",
    "            max_emotion = update_max_emotion(rounded_scores) \n",
    "        \n",
    "        plot_heatmap(x, y, w, h, cam, pil_crop_img, video_frame)\n",
    "        print_max_emotion(x, y, max_emotion, video_frame) # displays the max_emotion according to evaluation_frequency\n",
    "        print_all_emotion(x, y, w, rounded_scores, video_frame) # evaluates every video_frame for debugging\n",
    "        # draw bounding box on face\n",
    "        cv2.rectangle(video_frame, (x, y), (x + w, y + h), font_color, 2)\n",
    "\n",
    "    return faces\n",
    "\n",
    "cam_or_video = 'camera'\n",
    "video_path = 'test_video/test_video_noemotions02.mp4'\n",
    "\n",
    "def create_video(output_file='eval_video.mp4'):\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    fps = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
    "    frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    # output_path = 'eval_video.avi'\n",
    "    out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
    "    return out, video_capture \n",
    "\n",
    "# def create_webcam_video(output_file='output_video.avi', duration_seconds=10):\n",
    "def create_cam_video(output_file='cam_eval_video.mp4'):\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    frame_width = int(video_capture.get(3))\n",
    "    frame_height = int(video_capture.get(4))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_file, fourcc, 10.0, (frame_width, frame_height))\n",
    "    print(out.isOpened())\n",
    "    return out, video_capture\n",
    "\n",
    "\n",
    "if cam_or_video == 'camera':\n",
    "    out, video_capture = create_cam_video()\n",
    "elif cam_or_video == 'video':\n",
    "    out, video_capture = create_video()\n",
    "else: \n",
    "    print('unknown input')\n",
    "    print('please enter camera or video')\n",
    "\n",
    "counter = 0\n",
    "evaluation_frequency = 5\n",
    "\n",
    "# loop for Real-Time Face Detection\n",
    "while True:\n",
    "\n",
    "    result, video_frame = video_capture.read()  # read frames from the video\n",
    "    if result is False:\n",
    "        break  # terminate the loop if the frame is not read successfully\n",
    "    \n",
    "    faces = detect_bounding_box(video_frame, counter)  # apply the function we created to the video frame, faces as variable not used\n",
    "    \n",
    "    cv2.imshow(\"My Face Detection Project\", video_frame)  # display the processed frame in a window named \"My Face Detection Project\"\n",
    "\n",
    "    out.write(video_frame)  # write the processed frame to the output video file\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "    \n",
    "    counter += 1\n",
    "    if counter == evaluation_frequency:\n",
    "        counter = 0\n",
    "\n",
    "hook.unregister_hook()        \n",
    "video_capture.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def create_webcam_video(output_file='output_video.avi', duration_seconds=10):\n",
    "# def create_webcam_video(output_file='webcam_eval_video.avi'):\n",
    "#     # Open the webcam (default camera, 0)\n",
    "#     cap = cv2.VideoCapture(0)\n",
    "\n",
    "#     # fps_start_time = 0\n",
    "#     # fps = 0\n",
    "    \n",
    "#     # Get the default Width and Height of the frames\n",
    "#     frame_width = int(cap.get(3))\n",
    "#     frame_height = int(cap.get(4))\n",
    "\n",
    "#     # Define the codec and create a VideoWriter object\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#     out = cv2.VideoWriter(output_file, fourcc, 23.0, (frame_width, frame_height))\n",
    "\n",
    "#     # # Get the start time\n",
    "#     # start_time = time.time()\n",
    "\n",
    "#     # Capture video for the specified duration\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "\n",
    "#         # fps_end_time = time.time()\n",
    "\n",
    "#         # Write the frame into the file\n",
    "#         out.write(frame)\n",
    "\n",
    "#         # Display the resulting frame\n",
    "#         cv2.imshow('Webcam Video', frame)\n",
    "\n",
    "#         # # Check if the specified duration has passed\n",
    "#         # elapsed_time = time.time() - start_time\n",
    "#         # if elapsed_time >= duration_seconds:\n",
    "#         #     break\n",
    "\n",
    "#         # Break the loop if 'q' key is pressed\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "#     # Release everything when the job is finished\n",
    "#     cap.release()\n",
    "#     out.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     create_webcam_video()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
