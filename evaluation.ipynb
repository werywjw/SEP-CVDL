{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, Sequential, Module\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from model import EmotionClassifier\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12, 0.73, 0.04, 0.02, 0.02, 0.08]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class_labels = ['happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear']\n",
    "\n",
    "model = EmotionClassifier().to(device)\n",
    "model.load_state_dict(torch.load('best_RAF.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def classify_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    image_array = np.array(image)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "    scores = probabilities.cpu().numpy().flatten()\n",
    "    rounded_scores = [round(score, 2) for score in scores]\n",
    "    return rounded_scores, image, image_array, image_tensor\n",
    "\n",
    "# test_path = 'dataset/vali/img381_happiness.jpg'\n",
    "# test_path = 'dataset/vali/img005_surprise.jpg'\n",
    "test_path = 'dataset/vali/img113_fear.jpg'\n",
    "scores, image, image_array, image_tensor = classify_image(test_path)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # approach with self defined hook functions: https://towardsdatascience.com/grad-cam-in-pytorch-use-of-forward-and-backward-hooks-7eba5e38d569\n",
    "# # defines two global scope variables to store our gradients and activations\n",
    "# gradients = None\n",
    "# activations = None\n",
    "\n",
    "# def backward_hook(model, grad_input, grad_output):\n",
    "#   global gradients # refers to the variable in the global scope\n",
    "#   print('Backward hook running...')\n",
    "#   gradients = grad_output\n",
    "#   # In this case, we expect it to be torch.Size([batch size, 1024, 8, 8])\n",
    "#   print(f'Gradients size: {gradients[0].size()}') \n",
    "#   # We need the 0 index because the tensor containing the gradients comes\n",
    "#   # inside a one element tuple.\n",
    "\n",
    "# def forward_hook(model, args, output):\n",
    "#   global activations # refers to the variable in the global scope\n",
    "#   print('Forward hook running...')\n",
    "#   activations = output\n",
    "#   # In this case, we expect it to be torch.Size([batch size, 1024, 8, 8])\n",
    "#   print(f'Activations size: {activations.size()}')\n",
    "  \n",
    "# # backward_hook = model.bn5.register_full_backward_hook(backward_hook, prepend=False)\n",
    "# # forward_hook = model.bn5.register_forward_hook(forward_hook, prepend=False)\n",
    "\n",
    "# img_tensor = transform(image)\n",
    "# model(img_tensor.unsqueeze(0))\n",
    "# model.backward()\n",
    "\n",
    "# backward_hook.remove()\n",
    "# forward_hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # approach with pytorch Grad-CAM: https://github.com/jacobgil/pytorch-grad-cam?tab=readme-ov-file\n",
    "\n",
    "# print(model.bn5)\n",
    "# target_layers = [model.bn5]\n",
    "\n",
    "# # convert the input image to np.float32\n",
    "# image_array = image_array.astype(np.float32)\n",
    "\n",
    "# # normalize the input image to the range [0, 1]\n",
    "# image_array /= 255.0\n",
    "\n",
    "# # Print the shape of the array\n",
    "# print(\"Shape of image array:\", image_array.shape)\n",
    "\n",
    "# # construct the CAM object once, and then re-use it on many images:\n",
    "# cam = GradCAM(model=model, target_layers=target_layers)\n",
    "# print(cam)\n",
    "\n",
    "# targets = [ClassifierOutputTarget(1)]\n",
    "\n",
    "# # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "# grayscale_cam = cam(input_tensor=image_tensor, targets=targets)\n",
    "\n",
    "# # In this example grayscale_cam has only one image in the batch:\n",
    "# grayscale_cam = grayscale_cam[0, :]\n",
    "# visualization = show_cam_on_image(image_array, grayscale_cam, use_rgb=True)\n",
    "\n",
    "# print(type(visualization))\n",
    "# print(visualization.shape)\n",
    "\n",
    "\n",
    "# # Overlay the second image with transparency\n",
    "# plt.imshow(image2, alpha=0.5)\n",
    "\n",
    "# # Hide axes\n",
    "# plt.axis('off')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n",
    "\n",
    "# # Display the input image\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.imshow(image_array)\n",
    "# plt.title('Input Image')\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# # Display the CAM visualization\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.imshow(visualization)\n",
    "# plt.title('CAM Visualization')\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM with Torchcam: https://github.com/frgfm/torch-cam/blob/main/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bounding Box on Landmarks with OpenCV\n",
    "# # Rectangle not visibable\n",
    "# img = cv2.imread('test.jpg')\n",
    "# # img.shape\n",
    "# gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# # gray_image.shape\n",
    "\n",
    "# # load the classifier\n",
    "# face_classifier = cv2.CascadeClassifier(\n",
    "#     cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# # perform face detection\n",
    "# face = face_classifier.detectMultiScale(\n",
    "#     gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(40, 40))\n",
    "\n",
    "# # bounding box\n",
    "# for (x, y, w, h) in face:\n",
    "#     cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "# img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.imshow(img_rgb)\n",
    "# plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV Real-Time Face Detection\n",
    "## class_labels = ['happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear']\n",
    "\n",
    "import numpy as np\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# access webcam as numpy.ndarray\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "def detect_emotion(video_frame):\n",
    "    vid_fr_tensor = transform(video_frame).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(vid_fr_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "    scores = probabilities.cpu().numpy().flatten()\n",
    "    rounded_scores = [round(score, 2) for score in scores]\n",
    "    # print(f'rounded_scores in detect_emotion {rounded_scores}')\n",
    "    return rounded_scores\n",
    "\n",
    "# identify Face in Video Stream\n",
    "def detect_bounding_box(video_frame):\n",
    "    gray_image = cv2.cvtColor(video_frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "    for (x, y, w, h) in faces:\n",
    "        # draw bounding box on face\n",
    "        cv2.rectangle(video_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        # crop bounding box\n",
    "        crop_img = video_frame[y : y + h, x : x + w]\n",
    "        pil_crop_img = Image.fromarray(crop_img)\n",
    "        rounded_scores = detect_emotion(pil_crop_img)\n",
    "        # print(f'rounded_scores in detect_bounding_box: {rounded_scores}')\n",
    "        \n",
    "        # create text to be displayed\n",
    "        emotion_evaluation_str = []\n",
    "        for index, value in enumerate(class_labels):\n",
    "            emotion_evaluation_str.append(f'{value}: {rounded_scores[index]:.2f}')\n",
    "            \n",
    "        # get index from max value in rounded_scores\n",
    "        max_index = np.argmax(rounded_scores)\n",
    "        max_emotion = class_labels[max_index]\n",
    "\n",
    "        # text settings\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 1\n",
    "        font_color = (0, 255, 0)  # BGR color\n",
    "        thickness = 2\n",
    "        line_type = cv2.LINE_AA\n",
    "        # line_height = 40\n",
    "        \n",
    "        # position to put the text for the max emotion\n",
    "        org = (x, y - 15)\n",
    "        cv2.putText(video_frame, max_emotion, org, font, font_scale, font_color, thickness, line_type)\n",
    "        \n",
    "        # position to put the text for 6 emotions\n",
    "        # org = (x + w + 10, y + 20)\n",
    "        # cv2.putText(video_frame, max_emotion, org, font, font_scale, font_color, thickness, line_type)\n",
    "        \n",
    "        # # put each line of text on the image\n",
    "        # for i, line in enumerate(emotion_evaluation_str):\n",
    "        #     # Calculate the position for this line\n",
    "        #     y = org[1] + i * line_height\n",
    "\n",
    "    return faces\n",
    "\n",
    "# Loop for Real-Time Face Detection\n",
    "while True:\n",
    "\n",
    "    result, video_frame = video_capture.read()  # read frames from the video\n",
    "    if result is False:\n",
    "        break  # terminate the loop if the frame is not read successfully\n",
    "    # print(type(video_frame))\n",
    "    \n",
    "    faces = detect_bounding_box(\n",
    "        video_frame\n",
    "    )  # apply the function we created to the video frame\n",
    "\n",
    "    cv2.imshow(\n",
    "        \"My Face Detection Project\", video_frame\n",
    "    )  # display the processed frame in a window named \"My Face Detection Project\"\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
