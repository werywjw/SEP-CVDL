{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, Sequential, Module\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from model import EmotionClassifier\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12, 0.73, 0.04, 0.02, 0.02, 0.08]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class_labels = ['happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear']\n",
    "\n",
    "model = EmotionClassifier().to(device)\n",
    "model.load_state_dict(torch.load('best_RAF.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def classify_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    image_array = np.array(image)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "    scores = probabilities.cpu().numpy().flatten()\n",
    "    rounded_scores = [round(score, 2) for score in scores]\n",
    "    return rounded_scores, image, image_array, image_tensor\n",
    "\n",
    "# test_path = 'dataset/vali/img381_happiness.jpg'\n",
    "# test_path = 'dataset/vali/img005_surprise.jpg'\n",
    "test_path = 'dataset/vali/img113_fear.jpg'\n",
    "scores, image, image_array, image_tensor = classify_image(test_path)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cascade Landmarks, reading images / videos\n",
    "\n",
    "# # OpenCV Real-Time Face Detection\n",
    "# ## class_labels = ['happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear']\n",
    "\n",
    "# face_classifier = cv2.CascadeClassifier(\n",
    "#     cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# # access webcam as numpy.ndarray\n",
    "# video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# # text settings\n",
    "# font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "# font_scale = 1\n",
    "# font_color = (0, 255, 0)  # BGR color\n",
    "# thickness = 2\n",
    "# line_type = cv2.LINE_AA\n",
    "\n",
    "# max_emotion = ''\n",
    "\n",
    "# def detect_emotion(video_frame):\n",
    "#     vid_fr_tensor = transform(video_frame).unsqueeze(0).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(vid_fr_tensor)\n",
    "#         probabilities = F.softmax(outputs, dim=1)\n",
    "#     scores = probabilities.cpu().numpy().flatten()\n",
    "#     rounded_scores = [round(score, 2) for score in scores]\n",
    "#     # print(f'rounded_scores in detect_emotion {rounded_scores}')\n",
    "#     return rounded_scores\n",
    "\n",
    "# def get_max_emotion(x, y, w, h, video_frame):\n",
    "#     crop_img = video_frame[y : y + h, x : x + w]\n",
    "#     pil_crop_img = Image.fromarray(crop_img)\n",
    "#     # slower cropping\n",
    "#     rounded_scores = detect_emotion(pil_crop_img)    \n",
    "#     # get index from max value in rounded_scores\n",
    "#     max_index = np.argmax(rounded_scores)\n",
    "#     max_emotion = class_labels[max_index]\n",
    "#     # print(f'max_emotion: {max_emotion}')\n",
    "\n",
    "#     return max_emotion\n",
    "\n",
    "# def print_max_emotion(x, y, video_frame, max_emotion):\n",
    "#     # position to put the text for the max emotion\n",
    "#     org = (x, y - 15)\n",
    "#     cv2.putText(video_frame, max_emotion, org, font, font_scale, font_color, thickness, line_type)\n",
    "    \n",
    "# def print_all_emotion(x, y, w, h, video_frame):\n",
    "#     crop_img = video_frame[y : y + h, x : x + w]\n",
    "#     pil_crop_img = Image.fromarray(crop_img)\n",
    "#     # slower cropping\n",
    "#     rounded_scores = detect_emotion(pil_crop_img)\n",
    "#     # print(f'rounded_scores in detect_bounding_box: {rounded_scores}')\n",
    "#     # create text to be displayed\n",
    "#     org = (x + w + 10, y - 20)\n",
    "#     for index, value in enumerate(class_labels):\n",
    "#         emotion_str = (f'{value}: {rounded_scores[index]:.2f}')\n",
    "#         y = org[1] + 40\n",
    "#         org = (org[0], y)\n",
    "#         cv2.putText(video_frame, emotion_str, org, font, font_scale, font_color, thickness, line_type)\n",
    "    \n",
    "# # identify Face in Video Stream\n",
    "# def detect_bounding_box(video_frame, counter):\n",
    "#     global max_emotion\n",
    "#     gray_image = cv2.cvtColor(video_frame, cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         # draw bounding box on face\n",
    "#         cv2.rectangle(video_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "#         # crop bounding box\n",
    "#         if counter == 0:\n",
    "#             max_emotion = get_max_emotion(x, y, w, h, video_frame) \n",
    "        \n",
    "#         print_max_emotion(x, y, video_frame, max_emotion) # displays the max_emotion according to evaluation_frequency\n",
    "#         print_all_emotion(x, y, w, h, video_frame) # evaluates every video_frame for debugging\n",
    "\n",
    "#     return faces\n",
    "\n",
    "# counter = 0\n",
    "# evaluation_frequency = 5\n",
    "\n",
    "# # Loop for Real-Time Face Detection\n",
    "# while True:\n",
    "\n",
    "#     result, video_frame = video_capture.read()  # read frames from the video\n",
    "#     if result is False:\n",
    "#         break  # terminate the loop if the frame is not read successfully\n",
    "    \n",
    "#     faces = detect_bounding_box(video_frame, counter)  # apply the function we created to the video frame, faces as variable not used\n",
    "    \n",
    "#     cv2.imshow(\"My Face Detection Project\", video_frame)  # display the processed frame in a window named \"My Face Detection Project\"\n",
    "\n",
    "#     # print(type(video_frame))\n",
    "#     if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "#         break\n",
    "    \n",
    "#     counter += 1\n",
    "#     if counter == evaluation_frequency:\n",
    "#         counter = 0\n",
    "        \n",
    "# video_capture.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "# LBF Landmarks, saving video\n",
    "\n",
    "# OpenCV Real-Time Face Detection\n",
    "## class_labels = ['happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear']\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# text settings\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 1\n",
    "font_color = (145, 80, 255)  # BGR color\n",
    "thickness = 2\n",
    "line_type = cv2.LINE_AA\n",
    "\n",
    "max_emotion = ''\n",
    "\n",
    "def detect_emotion(video_frame):\n",
    "    vid_fr_tensor = transform(video_frame).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(vid_fr_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "    scores = probabilities.cpu().numpy().flatten()\n",
    "    rounded_scores = [round(score, 2) for score in scores]\n",
    "    # print(f'rounded_scores in detect_emotion {rounded_scores}')\n",
    "    return rounded_scores\n",
    "\n",
    "def update_max_emotion(rounded_scores):  \n",
    "    # get index from max value in rounded_scores\n",
    "    max_index = np.argmax(rounded_scores)\n",
    "    max_emotion = class_labels[max_index]\n",
    "    return max_emotion # returns max_emotion as string\n",
    "\n",
    "def print_max_emotion(x, y, video_frame, max_emotion):\n",
    "    # position to put the text for the max emotion\n",
    "    org = (x, y - 15)\n",
    "    cv2.putText(video_frame, max_emotion, org, font, font_scale, font_color, thickness, line_type)\n",
    "    \n",
    "def print_all_emotion(x, y, w, rounded_scores, video_frame):\n",
    "    # create text to be displayed\n",
    "    org = (x + w + 10, y - 20)\n",
    "    for index, value in enumerate(class_labels):\n",
    "        emotion_str = (f'{value}: {rounded_scores[index]:.2f}')\n",
    "        y = org[1] + 40\n",
    "        org = (org[0], y)\n",
    "        cv2.putText(video_frame, emotion_str, org, font, font_scale, font_color, thickness, line_type)\n",
    "    \n",
    "# identify Face in Video Stream\n",
    "def detect_bounding_box(video_frame, counter):\n",
    "    global max_emotion\n",
    "    gray_image = cv2.cvtColor(video_frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "    for (x, y, w, h) in faces:\n",
    "        crop_img = video_frame[y : y + h, x : x + w]\n",
    "        pil_crop_img = Image.fromarray(crop_img)\n",
    "        rounded_scores = detect_emotion(pil_crop_img)  \n",
    "        # cam = get_cam(w, h, pil_crop_img)\n",
    "        # plot(x, y, video_frame, cam)\n",
    "        # crop bounding box\n",
    "        if counter == 0:\n",
    "            max_emotion = update_max_emotion(rounded_scores) \n",
    "        \n",
    "        print_max_emotion(x, y, video_frame, max_emotion) # displays the max_emotion according to evaluation_frequency\n",
    "        print_all_emotion(x, y, w, rounded_scores, video_frame) # evaluates every video_frame for debugging\n",
    "        # draw bounding box on face\n",
    "        cv2.rectangle(video_frame, (x, y), (x + w, y + h), font_color, 2)\n",
    "\n",
    "    return faces\n",
    "\n",
    "\n",
    "cam_or_video = 'video'\n",
    "video_path = 'test_video/test_video_noemotions.mp4'\n",
    "\n",
    "if cam_or_video == 'cam':\n",
    "    # access webcam as numpy.ndarray\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "elif cam_or_video == 'video':\n",
    "    # Access the video file\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "else: \n",
    "    print('unknown input')\n",
    "    print('please enter cam or video')\n",
    "\n",
    "counter = 0\n",
    "evaluation_frequency = 5\n",
    "\n",
    "# Set up video writer\n",
    "fps = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_path = 'output.mp4'\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# loop for Real-Time Face Detection\n",
    "while True:\n",
    "\n",
    "    result, video_frame = video_capture.read()  # read frames from the video\n",
    "    if result is False:\n",
    "        break  # terminate the loop if the frame is not read successfully\n",
    "    \n",
    "    faces = detect_bounding_box(video_frame, counter)  # apply the function we created to the video frame, faces as variable not used\n",
    "    \n",
    "    cv2.imshow(\"My Face Detection Project\", video_frame)  # display the processed frame in a window named \"My Face Detection Project\"\n",
    "\n",
    "    out.write(video_frame)  # write the processed frame to the output video file\n",
    "    # print(type(video_frame))\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "    \n",
    "    counter += 1\n",
    "    if counter == evaluation_frequency:\n",
    "        counter = 0\n",
    "        \n",
    "video_capture.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
