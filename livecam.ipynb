{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, Sequential, Module\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from model import EmotionClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "class_labels = ['happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear']\n",
    "\n",
    "model = EmotionClassifier().to(device)\n",
    "model.load_state_dict(torch.load('best_RAF.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM with Torchcam: https://github.com/frgfm/torch-cam/blob/main/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 81\u001b[0m\n\u001b[1;32m     73\u001b[0m     faces \u001b[38;5;241m=\u001b[39m detect_bounding_box(\n\u001b[1;32m     74\u001b[0m         video_frame\n\u001b[1;32m     75\u001b[0m     )  \u001b[38;5;66;03m# apply the function we created to the video frame\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy Face Detection Project\u001b[39m\u001b[38;5;124m\"\u001b[39m, video_frame\n\u001b[1;32m     79\u001b[0m     )  \u001b[38;5;66;03m# display the processed frame in a window named \"My Face Detection Project\"\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     84\u001b[0m video_capture\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# access webcam as numpy.ndarray\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "def detect_emotion(video_frame):\n",
    "    vid_fr_tensor = transform(video_frame).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(vid_fr_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "    scores = probabilities.cpu().numpy().flatten()\n",
    "    rounded_scores = [round(score, 2) for score in scores]\n",
    "    # print(f'rounded_scores in detect_emotion {rounded_scores}')\n",
    "    return rounded_scores\n",
    "\n",
    "# identify Face in Video Stream\n",
    "def detect_bounding_box(video_frame):\n",
    "    gray_image = cv2.cvtColor(video_frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "    for (x, y, w, h) in faces:\n",
    "        # draw bounding box on face\n",
    "        cv2.rectangle(video_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        # crop bounding box\n",
    "        crop_img = video_frame[y : y + h, x : x + w]\n",
    "        pil_crop_img = Image.fromarray(crop_img)\n",
    "        rounded_scores = detect_emotion(pil_crop_img)\n",
    "        # print(f'rounded_scores in detect_bounding_box: {rounded_scores}')\n",
    "        \n",
    "        # create text to be displayed\n",
    "        emotion_evaluation_str = []\n",
    "        for index, value in enumerate(class_labels):\n",
    "            emotion_evaluation_str.append(f'{value}: {rounded_scores[index]:.2f}')\n",
    "            \n",
    "        # get index from max value in rounded_scores\n",
    "        max_index = np.argmax(rounded_scores)\n",
    "        max_emotion = class_labels[max_index]\n",
    "\n",
    "        # text settings\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 1\n",
    "        font_color = (0, 255, 0)  # BGR color\n",
    "        thickness = 2\n",
    "        line_type = cv2.LINE_AA\n",
    "        # line_height = 40\n",
    "        \n",
    "        # position to put the text for the max emotion\n",
    "        org = (x, y - 15)\n",
    "        cv2.putText(video_frame, max_emotion, org, font, font_scale, font_color, thickness, line_type)\n",
    "        \n",
    "        # position to put the text for 6 emotions\n",
    "        # org = (x + w + 10, y + 20)\n",
    "        # cv2.putText(video_frame, max_emotion, org, font, font_scale, font_color, thickness, line_type)\n",
    "        \n",
    "        # # put each line of text on the image\n",
    "        # for i, line in enumerate(emotion_evaluation_str):\n",
    "        #     # Calculate the position for this line\n",
    "        #     y = org[1] + i * line_height\n",
    "\n",
    "    return faces\n",
    "\n",
    "# Loop for Real-Time Face Detection\n",
    "while True:\n",
    "\n",
    "    result, video_frame = video_capture.read()  # read frames from the video\n",
    "    if result is False:\n",
    "        break  # terminate the loop if the frame is not read successfully\n",
    "    # print(type(video_frame))\n",
    "    \n",
    "    faces = detect_bounding_box(\n",
    "        video_frame\n",
    "    )  # apply the function we created to the video frame\n",
    "\n",
    "    cv2.imshow(\n",
    "        \"My Face Detection Project\", video_frame\n",
    "    )  # display the processed frame in a window named \"My Face Detection Project\"\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
