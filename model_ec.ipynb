{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, Sequential, Module\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class RAFDBDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.labels = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.img_dir, self.labels.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        label = self.labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch: image shape torch.Size([16, 3, 64, 64]), labels shape torch.Size([16])\n",
      "Vali batch: image shape torch.Size([16, 3, 64, 64]), labels shape torch.Size([16])\n",
      "Test batch: image shape torch.Size([16, 3, 64, 64]), labels shape torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "from rafdb_dataset import RAFDBDataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # transforms.RandomErasing(scale=(0.02,0.25)),\n",
    "])\n",
    "    \n",
    "rafdb_dataset_train = RAFDBDataset(csv_file='archive/RAF-DB/train_RAF_labels.csv',\n",
    "                            img_dir='archive/RAF-DB/train/',\n",
    "                            transform=transform)\n",
    "# rafdb_dataset_train = RAFDBDataset(csv_file='archive/FER+/train_FER_labels.csv',\n",
    "#                             img_dir='archive/FER+/train/',\n",
    "#                             transform=transform)\n",
    "data_train_loader = DataLoader(rafdb_dataset_train, batch_size=16, shuffle=True, num_workers=4)\n",
    "train_image, train_label = next(iter(data_train_loader))\n",
    "print(f\"Train batch: image shape {train_image.shape}, labels shape {train_label.shape}\")\n",
    "\n",
    "rafdb_dataset_vali = RAFDBDataset(csv_file='dataset/vali_labels.csv',\n",
    "                            img_dir='dataset/vali',\n",
    "                            transform=transform)\n",
    "data_vali_loader = DataLoader(rafdb_dataset_vali, batch_size=16, shuffle=False, num_workers=0)\n",
    "vali_image, vali_label = next(iter(data_vali_loader))\n",
    "print(f\"Vali batch: image shape {vali_image.shape}, labels shape {vali_label.shape}\")\n",
    "\n",
    "rafdb_dataset_test = RAFDBDataset(csv_file='archive/RAF-DB/test_RAF_labels.csv',\n",
    "                            img_dir='archive/RAF-DB/test/',\n",
    "                            transform=transform)\n",
    "# rafdb_dataset_test = RAFDBDataset(csv_file='archive/FER+/test_FER_labels.csv',\n",
    "#                             img_dir='archive/FER+/test/',\n",
    "#                             transform=transform)\n",
    "data_test_loader = DataLoader(rafdb_dataset_test, batch_size=16, shuffle=False, num_workers=0)\n",
    "test_image, test_label = next(iter(data_test_loader))\n",
    "print(f\"Test batch: image shape {test_image.shape}, labels shape {test_label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module): # Squeeze-and-Excitation (SE) blocks apply channel-wise attention.\n",
    "    def __init__(self, input_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_channels, input_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(input_channels // reduction, input_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)  \n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual \n",
    "# class EmotionClassifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(EmotionClassifier, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.se1 = SEBlock(64)\n",
    "\n",
    "#         # Using Residual Blocks\n",
    "#         self.res_block1 = ResidualBlock(64, 128, stride=2)\n",
    "#         self.res_block2 = ResidualBlock(128, 256, stride=2)\n",
    "#         self.res_block3 = ResidualBlock(256, 512, stride=2)\n",
    "#         self.res_block4 = ResidualBlock(512, 1024, stride=2)\n",
    "\n",
    "#         self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.fc1 = nn.Linear(1024, 2048)\n",
    "#         self.fc2 = nn.Linear(2048, 1024) \n",
    "#         self.dropout1 = nn.Dropout(0.5)\n",
    "#         self.fc3 = nn.Linear(1024, 6)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.bn1(self.conv1(x)))\n",
    "#         x = self.se1(x)\n",
    "        \n",
    "#         x = self.res_block1(x)\n",
    "#         x = self.res_block2(x)\n",
    "#         x = self.res_block3(x)\n",
    "#         x = self.res_block4(x)\n",
    "        \n",
    "#         x = self.pool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout1(x)\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "    \n",
    "# model = EmotionClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(1024)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(1024, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 1024) \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(1024, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = EmotionClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('best_baseline.pth', map_location=device))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 10478086\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class BasicBlock(nn.Module):\n",
    "#     expansion = 1\n",
    "\n",
    "#     def __init__(self, in_planes, planes, stride=1):\n",
    "#         super(BasicBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(planes)\n",
    "#         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "#         self.shortcut = nn.Sequential()\n",
    "#         if stride != 1 or in_planes != self.expansion * planes:\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(self.expansion * planes)\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.bn1(self.conv1(x)))\n",
    "#         out = self.bn2(self.conv2(out))\n",
    "#         out += self.shortcut(x)\n",
    "#         out = F.relu(out)\n",
    "#         return out\n",
    "\n",
    "# class ResNet(nn.Module):\n",
    "#     def __init__(self, block, num_blocks, num_classes=6):\n",
    "#         super(ResNet, self).__init__()\n",
    "#         self.in_planes = 64\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "#         self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "#         self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "#         self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "#         self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "#     def _make_layer(self, block, planes, num_blocks, stride):\n",
    "#         strides = [stride] + [1]*(num_blocks-1)\n",
    "#         layers = []\n",
    "#         for stride in strides:\n",
    "#             layers.append(block(self.in_planes, planes, stride))\n",
    "#             self.in_planes = planes * block.expansion\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.bn1(self.conv1(x)))\n",
    "#         out = self.layer1(out)\n",
    "#         out = self.layer2(out)\n",
    "#         out = self.layer3(out)\n",
    "#         out = self.layer4(out)\n",
    "#         out = self.avgpool(out)\n",
    "#         out = out.view(out.size(0), -1)\n",
    "#         out = self.fc(out)\n",
    "#         return out\n",
    "\n",
    "# def EmotionClassifierResNet18():\n",
    "#     return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "# model = EmotionClassifierResNet18().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class VGGEmotionClassifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(VGGEmotionClassifier, self).__init__()\n",
    "        \n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "#             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "#             nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(16384, 4096), \n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(4096, 1024),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(1024, 6)  \n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "# model = VGGEmotionClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'lr': [0.1, 0.01, 0.001, 0.0001], \n",
    "#     'batch_size': [8, 16, 32, 64],  \n",
    "# }\n",
    "# grid = ParameterGrid(param_grid)\n",
    "# results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for params in grid: # Hyperparameter tuning \n",
    "#     data_train_loader = DataLoader(rafdb_dataset_train, batch_size=params['batch_size'], shuffle=True, num_workers=4)\n",
    "#     data_vali_loader = DataLoader(rafdb_dataset_vali, batch_size=params['batch_size'], shuffle=False, num_workers=0)\n",
    "    \n",
    "#     model = EmotionClassifier().to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "#     best_val_acc = 0\n",
    "#     num_epochs = 15\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         for i, data in enumerate(tqdm(data_train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"), 0):\n",
    "#             inputs, labels = data[0].to(device), data[1].to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#     model.eval()\n",
    "#     val_correct = 0\n",
    "#     val_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data in data_vali_loader:\n",
    "#             inputs, labels = data[0].to(device), data[1].to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             val_total += labels.size(0)\n",
    "#             val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "#     val_acc = val_correct / val_total\n",
    "#     best_val_acc = max(best_val_acc, val_acc)\n",
    "    \n",
    "#     results.append({\n",
    "#         'lr': params['lr'],\n",
    "#         'batch_size': params['batch_size'],\n",
    "#         'best_val_acc': best_val_acc,\n",
    "#     })\n",
    "\n",
    "# for result in results:\n",
    "#     print(f\"LR: {result['lr']}, Batch Size: {result['batch_size']}, Best Val Acc: {result['best_val_acc']}\")\n",
    "\n",
    "# best_params = max(results, key=lambda x: x['best_val_acc'])\n",
    "# print(f\"Best params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) # test 5/0.5 later\n",
    "\n",
    "patience = 10\n",
    "best_val_acc = 0  \n",
    "patience_counter = 0\n",
    "\n",
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40: 100%|██████████| 610/610 [00:52<00:00, 11.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.6011785337182343, Train Accuracy: 0.4753745126205623, Test Loss: 1.4323044856389364, Test Accuracy: 0.49602010892333476, Validation Loss: 2.1661044861141003, Validation Accuracy: 0.1669449081803005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40: 100%|██████████| 610/610 [00:51<00:00, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 1.43622547319678, Train Accuracy: 0.489534167863739, Test Loss: 1.4353316036860149, Test Accuracy: 0.49602010892333476, Validation Loss: 2.156328609115199, Validation Accuracy: 0.1669449081803005\n",
      "No improvement in validation accuracy for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40: 100%|██████████| 610/610 [00:51<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 1.4324128637548352, Train Accuracy: 0.489534167863739, Test Loss: 1.4264061220486959, Test Accuracy: 0.49602010892333476, Validation Loss: 2.176705943910699, Validation Accuracy: 0.1669449081803005\n",
      "No improvement in validation accuracy for 2 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40: 100%|██████████| 610/610 [00:52<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 1.4347571664169187, Train Accuracy: 0.489534167863739, Test Loss: 1.4276211190223693, Test Accuracy: 0.49602010892333476, Validation Loss: 2.2001089014505086, Validation Accuracy: 0.1669449081803005\n",
      "No improvement in validation accuracy for 3 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40: 100%|██████████| 610/610 [00:52<00:00, 11.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 1.4323717862856193, Train Accuracy: 0.4893289554689103, Test Loss: 1.4281489141782124, Test Accuracy: 0.49602010892333476, Validation Loss: 2.108352447810926, Validation Accuracy: 0.1669449081803005\n",
      "No improvement in validation accuracy for 4 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40: 100%|██████████| 610/610 [00:52<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 1.4309221069343754, Train Accuracy: 0.489534167863739, Test Loss: 1.4281335695584616, Test Accuracy: 0.49602010892333476, Validation Loss: 2.2158012327394987, Validation Accuracy: 0.1669449081803005\n",
      "No improvement in validation accuracy for 5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40: 100%|██████████| 610/610 [00:52<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 1.4308527413938865, Train Accuracy: 0.489534167863739, Test Loss: 1.4274051475524903, Test Accuracy: 0.49602010892333476, Validation Loss: 2.1670648643845007, Validation Accuracy: 0.1669449081803005\n",
      "No improvement in validation accuracy for 6 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40: 100%|██████████| 610/610 [00:53<00:00, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 1.429504872247821, Train Accuracy: 0.489534167863739, Test Loss: 1.4268077023824055, Test Accuracy: 0.49602010892333476, Validation Loss: 2.1326606022684196, Validation Accuracy: 0.1669449081803005\n",
      "No improvement in validation accuracy for 7 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40: 100%|██████████| 610/610 [00:43<00:00, 13.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m tqdm(data_train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     15\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:1317\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1317\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shutdown_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \n\u001b[1;32m   1322\u001b[0m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:1442\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers:\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m     \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMP_STATUS_CHECK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues:\n\u001b[1;32m   1444\u001b[0m     q\u001b[38;5;241m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentinel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in tqdm(data_train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(data_train_loader)\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in data_test_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = test_running_loss / len(data_test_loader)\n",
    "    test_acc = test_correct / test_total\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in data_vali_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_running_loss / len(data_vali_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}, Train Accuracy: {train_acc}, Test Loss: {test_loss}, Test Accuracy: {test_acc}, Validation Loss: {val_loss}, Validation Accuracy: {val_acc}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0 \n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in validation accuracy for {patience_counter} epochs.\")\n",
    "    \n",
    "    if patience_counter > patience:\n",
    "        print(\"Stopping early due to lack of improvement in validation accuracy.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (37,) and (24,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m38\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrain Loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# change this number after '(1, _)' to num_epochs+1\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m38\u001b[39m), test_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# change this number after '(1, _)' to num_epochs+1\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m38\u001b[39m), val_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# change this number after '(1, _)' to num_epochs+1\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/pyplot.py:3575\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3567\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3569\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3573\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3574\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3576\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/axes/_axes.py:1721\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1721\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/axes/_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    302\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/axes/_base.py:499\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    496\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (37,) and (24,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk8AAAJMCAYAAADjU0LAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAf3ElEQVR4nO3df2zV9b348Vcp9lQzW9nlUn7cOq7uOrep4EB6qzNmS++aaNjlj5v16gJc4o/rxoyjuXeCIJ1zo1yvGm4mjsj0uj/mYDNqlkHwut6RxdkbMn4k7goahw7usla4u7Zc3FppP98/dq3fjuJ4VdrC+ngk54++fb/PeR/fsj3zOYdPy4qiKAIAgJMyYaw3AABwJhFPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACel4+vGPfxzz58+P6dOnR1lZWTz99NN/cM327dvjYx/7WJRKpfjgBz8Yjz322DC2CgAw9tLxdPTo0Zg1a1asX7/+pOa/+uqrcd1118UnPvGJ2LNnT3zxi1+Mm266KZ555pn0ZgEAxlrZe/nFwGVlZfHUU0/FggULTjjnjjvuiC1btsTPfvazgbG//du/jTfeeCO2bds23JcGABgTE0f6Bdrb26OhoWHQWGNjY3zxi1884Zqenp7o6ekZ+Lm/vz9+/etfx5/8yZ9EWVnZSG0VAPgjUhRFHDlyJKZPnx4TJpy6r3mPeDx1dHRETU3NoLGampro7u6O3/zmN3H22Wcft6a1tTXuvvvukd4aADAOHDx4MP7sz/7slD3fiMfTcKxYsSKam5sHfu7q6orzzz8/Dh48GFVVVWO4MwDgTNHd3R21tbVx7rnnntLnHfF4mjp1anR2dg4a6+zsjKqqqiGvOkVElEqlKJVKx41XVVWJJwAg5VR/5WfE7/NUX18fbW1tg8aeffbZqK+vH+mXBgA45dLx9L//+7+xZ8+e2LNnT0T87lYEe/bsiQMHDkTE7z5yW7Ro0cD8W2+9Nfbv3x9f+tKXYt++ffHQQw/Fd7/73Vi2bNmpeQcAAKMoHU8//elP4/LLL4/LL788IiKam5vj8ssvj9WrV0dExK9+9auBkIqI+PM///PYsmVLPPvsszFr1qy4//7745vf/GY0NjaeorcAADB63tN9nkZLd3d3VFdXR1dXl+88AQAnZaT6we+2AwBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIGFY8rV+/PmbOnBmVlZVRV1cXO3bseNf569atiw996ENx9tlnR21tbSxbtix++9vfDmvDAABjKR1Pmzdvjubm5mhpaYldu3bFrFmzorGxMV5//fUh5z/++OOxfPnyaGlpib1798YjjzwSmzdvjjvvvPM9bx4AYLSl4+mBBx6Im2++OZYsWRIf+chHYsOGDXHOOefEo48+OuT8559/Pq666qq44YYbYubMmfGpT30qrr/++j94tQoA4HSUiqfe3t7YuXNnNDQ0vPMEEyZEQ0NDtLe3D7nmyiuvjJ07dw7E0v79+2Pr1q1x7bXXnvB1enp6oru7e9ADAOB0MDEz+fDhw9HX1xc1NTWDxmtqamLfvn1Drrnhhhvi8OHD8fGPfzyKoohjx47Frbfe+q4f27W2tsbdd9+d2RoAwKgY8b9tt3379lizZk089NBDsWvXrnjyySdjy5Ytcc8995xwzYoVK6Krq2vgcfDgwZHeJgDASUldeZo8eXKUl5dHZ2fnoPHOzs6YOnXqkGvuuuuuWLhwYdx0000REXHppZfG0aNH45ZbbomVK1fGhAnH91upVIpSqZTZGgDAqEhdeaqoqIg5c+ZEW1vbwFh/f3+0tbVFfX39kGvefPPN4wKpvLw8IiKKosjuFwBgTKWuPEVENDc3x+LFi2Pu3Lkxb968WLduXRw9ejSWLFkSERGLFi2KGTNmRGtra0REzJ8/Px544IG4/PLLo66uLl555ZW46667Yv78+QMRBQBwpkjHU1NTUxw6dChWr14dHR0dMXv27Ni2bdvAl8gPHDgw6ErTqlWroqysLFatWhW//OUv40//9E9j/vz58bWvfe3UvQsAgFFSVpwBn511d3dHdXV1dHV1RVVV1VhvBwA4A4xUP/jddgAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACcOKp/Xr18fMmTOjsrIy6urqYseOHe86/4033oilS5fGtGnTolQqxUUXXRRbt24d1oYBAMbSxOyCzZs3R3Nzc2zYsCHq6upi3bp10djYGC+99FJMmTLluPm9vb3xV3/1VzFlypR44oknYsaMGfGLX/wizjvvvFOxfwCAUVVWFEWRWVBXVxdXXHFFPPjggxER0d/fH7W1tXHbbbfF8uXLj5u/YcOG+Od//ufYt29fnHXWWcPaZHd3d1RXV0dXV1dUVVUN6zkAgPFlpPoh9bFdb29v7Ny5MxoaGt55ggkToqGhIdrb24dc8/3vfz/q6+tj6dKlUVNTE5dcckmsWbMm+vr6Tvg6PT090d3dPegBAHA6SMXT4cOHo6+vL2pqagaN19TUREdHx5Br9u/fH0888UT09fXF1q1b46677or7778/vvrVr57wdVpbW6O6unrgUVtbm9kmAMCIGfG/bdff3x9TpkyJhx9+OObMmRNNTU2xcuXK2LBhwwnXrFixIrq6ugYeBw8eHOltAgCclNQXxidPnhzl5eXR2dk5aLyzszOmTp065Jpp06bFWWedFeXl5QNjH/7wh6OjoyN6e3ujoqLiuDWlUilKpVJmawAAoyJ15amioiLmzJkTbW1tA2P9/f3R1tYW9fX1Q6656qqr4pVXXon+/v6BsZdffjmmTZs2ZDgBAJzO0h/bNTc3x8aNG+Nb3/pW7N27Nz73uc/F0aNHY8mSJRERsWjRolixYsXA/M997nPx61//Om6//fZ4+eWXY8uWLbFmzZpYunTpqXsXAACjJH2fp6ampjh06FCsXr06Ojo6Yvbs2bFt27aBL5EfOHAgJkx4p8lqa2vjmWeeiWXLlsVll10WM2bMiNtvvz3uuOOOU/cuAABGSfo+T2PBfZ4AgKzT4j5PAADjnXgCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABKGFU/r16+PmTNnRmVlZdTV1cWOHTtOat2mTZuirKwsFixYMJyXBQAYc+l42rx5czQ3N0dLS0vs2rUrZs2aFY2NjfH666+/67rXXnst/uEf/iGuvvrqYW8WAGCspePpgQceiJtvvjmWLFkSH/nIR2LDhg1xzjnnxKOPPnrCNX19ffHZz3427r777rjgggve04YBAMZSKp56e3tj586d0dDQ8M4TTJgQDQ0N0d7efsJ1X/nKV2LKlClx4403Dn+nAACngYmZyYcPH46+vr6oqakZNF5TUxP79u0bcs1zzz0XjzzySOzZs+ekX6enpyd6enoGfu7u7s5sEwBgxIzo37Y7cuRILFy4MDZu3BiTJ08+6XWtra1RXV098KitrR3BXQIAnLzUlafJkydHeXl5dHZ2Dhrv7OyMqVOnHjf/5z//ebz22msxf/78gbH+/v7fvfDEifHSSy/FhRdeeNy6FStWRHNz88DP3d3dAgoAOC2k4qmioiLmzJkTbW1tA7cb6O/vj7a2tvjCF75w3PyLL744XnjhhUFjq1atiiNHjsS//Mu/nDCISqVSlEqlzNYAAEZFKp4iIpqbm2Px4sUxd+7cmDdvXqxbty6OHj0aS5YsiYiIRYsWxYwZM6K1tTUqKyvjkksuGbT+vPPOi4g4bhwA4EyQjqempqY4dOhQrF69Ojo6OmL27Nmxbdu2gS+RHzhwICZMcONyAOCPU1lRFMVYb+IP6e7ujurq6ujq6oqqqqqx3g4AcAYYqX5wiQgAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIGFY8bR+/fqYOXNmVFZWRl1dXezYseOEczdu3BhXX311TJo0KSZNmhQNDQ3vOh8A4HSWjqfNmzdHc3NztLS0xK5du2LWrFnR2NgYr7/++pDzt2/fHtdff3386Ec/ivb29qitrY1PfepT8ctf/vI9bx4AYLSVFUVRZBbU1dXFFVdcEQ8++GBERPT390dtbW3cdtttsXz58j+4vq+vLyZNmhQPPvhgLFq06KRes7u7O6qrq6Orqyuqqqoy2wUAxqmR6ofUlafe3t7YuXNnNDQ0vPMEEyZEQ0NDtLe3n9RzvPnmm/HWW2/F+9///hPO6enpie7u7kEPAIDTQSqeDh8+HH19fVFTUzNovKamJjo6Ok7qOe64446YPn36oAD7fa2trVFdXT3wqK2tzWwTAGDEjOrftlu7dm1s2rQpnnrqqaisrDzhvBUrVkRXV9fA4+DBg6O4SwCAE5uYmTx58uQoLy+Pzs7OQeOdnZ0xderUd1173333xdq1a+OHP/xhXHbZZe86t1QqRalUymwNAGBUpK48VVRUxJw5c6KtrW1grL+/P9ra2qK+vv6E6+6999645557Ytu2bTF37tzh7xYAYIylrjxFRDQ3N8fixYtj7ty5MW/evFi3bl0cPXo0lixZEhERixYtihkzZkRra2tERPzTP/1TrF69Oh5//PGYOXPmwHej3ve+98X73ve+U/hWAABGXjqempqa4tChQ7F69ero6OiI2bNnx7Zt2wa+RH7gwIGYMOGdC1rf+MY3ore3N/7mb/5m0PO0tLTEl7/85fe2ewCAUZa+z9NYcJ8nACDrtLjPEwDAeCeeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkCCeAAASxBMAQIJ4AgBIEE8AAAniCQAgQTwBACSIJwCABPEEAJAgngAAEsQTAECCeAIASBBPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJIgnAIAE8QQAkDCseFq/fn3MnDkzKisro66uLnbs2PGu87/3ve/FxRdfHJWVlXHppZfG1q1bh7VZAICxlo6nzZs3R3Nzc7S0tMSuXbti1qxZ0djYGK+//vqQ859//vm4/vrr48Ybb4zdu3fHggULYsGCBfGzn/3sPW8eAGC0lRVFUWQW1NXVxRVXXBEPPvhgRET09/dHbW1t3HbbbbF8+fLj5jc1NcXRo0fjBz/4wcDYX/7lX8bs2bNjw4YNJ/Wa3d3dUV1dHV1dXVFVVZXZLgAwTo1UP0zMTO7t7Y2dO3fGihUrBsYmTJgQDQ0N0d7ePuSa9vb2aG5uHjTW2NgYTz/99Alfp6enJ3p6egZ+7urqiojf/UsAADgZb3dD8jrRH5SKp8OHD0dfX1/U1NQMGq+pqYl9+/YNuaajo2PI+R0dHSd8ndbW1rj77ruPG6+trc1sFwAg/vu//zuqq6tP2fOl4mm0rFixYtDVqjfeeCM+8IEPxIEDB07pm+fU6e7ujtra2jh48KCPVk9jzunM4JxOf87ozNDV1RXnn39+vP/97z+lz5uKp8mTJ0d5eXl0dnYOGu/s7IypU6cOuWbq1Kmp+RERpVIpSqXScePV1dX+Iz3NVVVVOaMzgHM6Mzin058zOjNMmHBq78yUeraKioqYM2dOtLW1DYz19/dHW1tb1NfXD7mmvr5+0PyIiGefffaE8wEATmfpj+2am5tj8eLFMXfu3Jg3b16sW7cujh49GkuWLImIiEWLFsWMGTOitbU1IiJuv/32uOaaa+L++++P6667LjZt2hQ//elP4+GHHz617wQAYBSk46mpqSkOHToUq1evjo6Ojpg9e3Zs27Zt4EvhBw4cGHR57Morr4zHH388Vq1aFXfeeWf8xV/8RTz99NNxySWXnPRrlkqlaGlpGfKjPE4PzujM4JzODM7p9OeMzgwjdU7p+zwBAIxnfrcdAECCeAIASBBPAAAJ4gkAIOG0iaf169fHzJkzo7KyMurq6mLHjh3vOv973/teXHzxxVFZWRmXXnppbN26dZR2On5lzmjjxo1x9dVXx6RJk2LSpEnR0NDwB8+UUyP7Z+ltmzZtirKysliwYMHIbpCIyJ/TG2+8EUuXLo1p06ZFqVSKiy66yP/ujbDsGa1bty4+9KEPxdlnnx21tbWxbNmy+O1vfztKux2ffvzjH8f8+fNj+vTpUVZW9q6/N/dt27dvj4997GNRKpXigx/8YDz22GP5Fy5OA5s2bSoqKiqKRx99tPjP//zP4uabby7OO++8orOzc8j5P/nJT4ry8vLi3nvvLV588cVi1apVxVlnnVW88MILo7zz8SN7RjfccEOxfv36Yvfu3cXevXuLv/u7vyuqq6uL//qv/xrlnY8v2XN626uvvlrMmDGjuPrqq4u//uu/Hp3NjmPZc+rp6Snmzp1bXHvttcVzzz1XvPrqq8X27duLPXv2jPLOx4/sGX37298uSqVS8e1vf7t49dVXi2eeeaaYNm1asWzZslHe+fiydevWYuXKlcWTTz5ZRETx1FNPvev8/fv3F+ecc07R3NxcvPjii8XXv/71ory8vNi2bVvqdU+LeJo3b16xdOnSgZ/7+vqK6dOnF62trUPO/8xnPlNcd911g8bq6uqKv//7vx/RfY5n2TP6fceOHSvOPffc4lvf+tZIbZFieOd07Nix4sorryy++c1vFosXLxZPoyB7Tt/4xjeKCy64oOjt7R2tLY572TNaunRp8clPfnLQWHNzc3HVVVeN6D55x8nE05e+9KXiox/96KCxpqamorGxMfVaY/6xXW9vb+zcuTMaGhoGxiZMmBANDQ3R3t4+5Jr29vZB8yMiGhsbTzif92Y4Z/T73nzzzXjrrbdO+S9n5B3DPaevfOUrMWXKlLjxxhtHY5vj3nDO6fvf/37U19fH0qVLo6amJi655JJYs2ZN9PX1jda2x5XhnNGVV14ZO3fuHPhob//+/bF169a49tprR2XPnJxT1Q/pO4yfaocPH46+vr6BO5S/raamJvbt2zfkmo6OjiHnd3R0jNg+x7PhnNHvu+OOO2L69OnH/UfLqTOcc3ruuefikUceiT179ozCDokY3jnt378//v3f/z0++9nPxtatW+OVV16Jz3/+8/HWW29FS0vLaGx7XBnOGd1www1x+PDh+PjHPx5FUcSxY8fi1ltvjTvvvHM0tsxJOlE/dHd3x29+85s4++yzT+p5xvzKE3/81q5dG5s2bYqnnnoqKisrx3o7/J8jR47EwoULY+PGjTF58uSx3g7vor+/P6ZMmRIPP/xwzJkzJ5qammLlypWxYcOGsd4a/2f79u2xZs2aeOihh2LXrl3x5JNPxpYtW+Kee+4Z660xAsb8ytPkyZOjvLw8Ojs7B413dnbG1KlTh1wzderU1Hzem+Gc0dvuu+++WLt2bfzwhz+Myy67bCS3Oe5lz+nnP/95vPbaazF//vyBsf7+/oiImDhxYrz00ktx4YUXjuymx6Hh/HmaNm1anHXWWVFeXj4w9uEPfzg6Ojqit7c3KioqRnTP481wzuiuu+6KhQsXxk033RQREZdeemkcPXo0brnllli5cuWg3/nK2DlRP1RVVZ30VaeI0+DKU0VFRcyZMyfa2toGxvr7+6OtrS3q6+uHXFNfXz9ofkTEs88+e8L5vDfDOaOIiHvvvTfuueee2LZtW8ydO3c0tjquZc/p4osvjhdeeCH27Nkz8Pj0pz8dn/jEJ2LPnj1RW1s7mtsfN4bz5+mqq66KV155ZSBuIyJefvnlmDZtmnAaAcM5ozfffPO4QHo7dgu/Qva0ccr6Ifdd9pGxadOmolQqFY899ljx4osvFrfccktx3nnnFR0dHUVRFMXChQuL5cuXD8z/yU9+UkycOLG47777ir179xYtLS1uVTDCsme0du3aoqKionjiiSeKX/3qVwOPI0eOjNVbGBey5/T7/G270ZE9pwMHDhTnnntu8YUvfKF46aWXih/84AfFlClTiq9+9atj9Rb+6GXPqKWlpTj33HOL73znO8X+/fuLf/u3fysuvPDC4jOf+cxYvYVx4ciRI8Xu3buL3bt3FxFRPPDAA8Xu3buLX/ziF0VRFMXy5cuLhQsXDsx/+1YF//iP/1js3bu3WL9+/Zl7q4KiKIqvf/3rxfnnn19UVFQU8+bNK/7jP/5j4J9dc801xeLFiwfN/+53v1tcdNFFRUVFRfHRj3602LJlyyjvePzJnNEHPvCBIiKOe7S0tIz+xseZ7J+l/594Gj3Zc3r++eeLurq6olQqFRdccEHxta99rTh27Ngo73p8yZzRW2+9VXz5y18uLrzwwqKysrKora0tPv/5zxf/8z//M/obH0d+9KMfDfn/NW+fzeLFi4trrrnmuDWzZ88uKioqigsuuKD413/91/TrlhWF64kAACdrzL/zBABwJhFPAAAJ4gkAIEE8AQAkiCcAgATxBACQIJ4AABLEEwBAgngCAEgQTwAACeIJACBBPAEAJPw/SXW0t6YSQEUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, 38), train_losses, label='Train Loss') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.plot(range(1, 38), test_losses, label='Test Loss') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.plot(range(1, 38), val_losses, label='Validation Loss') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Losses on RAF-DB') # change\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 38), train_accuracies, label='Train Accuracy') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.plot(range(1, 38), test_accuracies, label='Test Accuracy') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.plot(range(1, 38), val_accuracies, label='Validation Accuracy') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracies on RAF-DB') # change\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Epoch': range(1, 18), # change this number after '(1, _)' to num_epochs+1\n",
    "    'Train Loss': train_losses,\n",
    "    'Test Loss': test_losses,\n",
    "    'Validation Loss': val_losses,\n",
    "    'Train Accuracy': train_accuracies,\n",
    "    'Test Accuracy': test_accuracies,\n",
    "    'Validation Accuracy': val_accuracies\n",
    "})\n",
    "df.to_csv('result_FER_4layer.csv', index=False) # change this CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
