{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, Sequential, Module\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class RAFDBDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.labels = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.img_dir, self.labels.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        label = self.labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch: image shape torch.Size([16, 3, 64, 64]), labels shape torch.Size([16])\n",
      "Vali batch: image shape torch.Size([16, 3, 64, 64]), labels shape torch.Size([16])\n",
      "Test batch: image shape torch.Size([16, 3, 64, 64]), labels shape torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "from rafdb_dataset import RAFDBDataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    # transforms.RandomHorizontalFlip(), \n",
    "    # transforms.RandomApply([\n",
    "    #     transforms.RandomRotation(5),\n",
    "    #     transforms.RandomCrop(64, padding=8)\n",
    "    # ], p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # transforms.RandomErasing(scale=(0.02,0.25)),\n",
    "])\n",
    "    \n",
    "rafdb_dataset_train = RAFDBDataset(csv_file='archive/RAF-DB/train_RAF_labels.csv',\n",
    "                            img_dir='archive/RAF-DB/train/',\n",
    "                            transform=transform)\n",
    "# rafdb_dataset_train = RAFDBDataset(csv_file='archive/FER+/train_FER_labels.csv',\n",
    "#                             img_dir='archive/FER+/train/',\n",
    "#                             transform=transform)\n",
    "data_train_loader = DataLoader(rafdb_dataset_train, batch_size=16, shuffle=True, num_workers=4)\n",
    "train_image, train_label = next(iter(data_train_loader))\n",
    "print(f\"Train batch: image shape {train_image.shape}, labels shape {train_label.shape}\")\n",
    "\n",
    "rafdb_dataset_vali = RAFDBDataset(csv_file='dataset/vali_labels.csv',\n",
    "                            img_dir='dataset/vali',\n",
    "                            transform=transform)\n",
    "data_vali_loader = DataLoader(rafdb_dataset_vali, batch_size=16, shuffle=False, num_workers=0)\n",
    "vali_image, vali_label = next(iter(data_vali_loader))\n",
    "print(f\"Vali batch: image shape {vali_image.shape}, labels shape {vali_label.shape}\")\n",
    "\n",
    "rafdb_dataset_test = RAFDBDataset(csv_file='archive/RAF-DB/test_RAF_labels.csv',\n",
    "                            img_dir='archive/RAF-DB/test/',\n",
    "                            transform=transform)\n",
    "# rafdb_dataset_test = RAFDBDataset(csv_file='archive/FER+/test_FER_labels.csv',\n",
    "#                             img_dir='archive/FER+/test/',\n",
    "#                             transform=transform)\n",
    "data_test_loader = DataLoader(rafdb_dataset_test, batch_size=16, shuffle=False, num_workers=0)\n",
    "test_image, test_label = next(iter(data_test_loader))\n",
    "print(f\"Test batch: image shape {test_image.shape}, labels shape {test_label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module): # Squeeze-and-Excitation (SE) blocks apply channel-wise attention.\n",
    "    def __init__(self, input_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_channels, input_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(input_channels // reduction, input_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)  \n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual \n",
    "# class EmotionClassifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(EmotionClassifier, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.se1 = SEBlock(64)\n",
    "\n",
    "#         # Using Residual Blocks\n",
    "#         self.res_block1 = ResidualBlock(64, 128, stride=2)\n",
    "#         self.res_block2 = ResidualBlock(128, 256, stride=2)\n",
    "#         self.res_block3 = ResidualBlock(256, 512, stride=2)\n",
    "#         self.res_block4 = ResidualBlock(512, 1024, stride=2)\n",
    "\n",
    "#         self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.fc1 = nn.Linear(1024, 2048)\n",
    "#         self.fc2 = nn.Linear(2048, 1024) \n",
    "#         self.dropout1 = nn.Dropout(0.5)\n",
    "#         self.fc3 = nn.Linear(1024, 6)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.bn1(self.conv1(x)))\n",
    "#         x = self.se1(x)\n",
    "        \n",
    "#         x = self.res_block1(x)\n",
    "#         x = self.res_block2(x)\n",
    "#         x = self.res_block3(x)\n",
    "#         x = self.res_block4(x)\n",
    "        \n",
    "#         x = self.pool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout1(x)\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "    \n",
    "# model = EmotionClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(1024)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(1024, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 1024) \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(1024, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = EmotionClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('best_baseline.pth', map_location=device))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 41950726\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class BasicBlock(nn.Module):\n",
    "#     expansion = 1\n",
    "\n",
    "#     def __init__(self, in_planes, planes, stride=1):\n",
    "#         super(BasicBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(planes)\n",
    "#         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "#         self.shortcut = nn.Sequential()\n",
    "#         if stride != 1 or in_planes != self.expansion * planes:\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(self.expansion * planes)\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.bn1(self.conv1(x)))\n",
    "#         out = self.bn2(self.conv2(out))\n",
    "#         out += self.shortcut(x)\n",
    "#         out = F.relu(out)\n",
    "#         return out\n",
    "\n",
    "# class ResNet(nn.Module):\n",
    "#     def __init__(self, block, num_blocks, num_classes=6):\n",
    "#         super(ResNet, self).__init__()\n",
    "#         self.in_planes = 64\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "#         self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "#         self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "#         self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "#         self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "#     def _make_layer(self, block, planes, num_blocks, stride):\n",
    "#         strides = [stride] + [1]*(num_blocks-1)\n",
    "#         layers = []\n",
    "#         for stride in strides:\n",
    "#             layers.append(block(self.in_planes, planes, stride))\n",
    "#             self.in_planes = planes * block.expansion\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.bn1(self.conv1(x)))\n",
    "#         out = self.layer1(out)\n",
    "#         out = self.layer2(out)\n",
    "#         out = self.layer3(out)\n",
    "#         out = self.layer4(out)\n",
    "#         out = self.avgpool(out)\n",
    "#         out = out.view(out.size(0), -1)\n",
    "#         out = self.fc(out)\n",
    "#         return out\n",
    "\n",
    "# def EmotionClassifierResNet18():\n",
    "#     return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "# model = EmotionClassifierResNet18().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class VGGEmotionClassifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(VGGEmotionClassifier, self).__init__()\n",
    "        \n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "#             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "#             nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(16384, 4096), \n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(4096, 1024),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(1024, 6)  \n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "# model = VGGEmotionClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'lr': [0.1, 0.01, 0.001, 0.0001], \n",
    "#     'batch_size': [8, 16, 32, 64],  \n",
    "# }\n",
    "# grid = ParameterGrid(param_grid)\n",
    "# results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for params in grid: # Hyperparameter tuning \n",
    "#     data_train_loader = DataLoader(rafdb_dataset_train, batch_size=params['batch_size'], shuffle=True, num_workers=4)\n",
    "#     data_vali_loader = DataLoader(rafdb_dataset_vali, batch_size=params['batch_size'], shuffle=False, num_workers=0)\n",
    "    \n",
    "#     model = EmotionClassifier().to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "#     best_val_acc = 0\n",
    "#     num_epochs = 15\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         for i, data in enumerate(tqdm(data_train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"), 0):\n",
    "#             inputs, labels = data[0].to(device), data[1].to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#     model.eval()\n",
    "#     val_correct = 0\n",
    "#     val_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data in data_vali_loader:\n",
    "#             inputs, labels = data[0].to(device), data[1].to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             val_total += labels.size(0)\n",
    "#             val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "#     val_acc = val_correct / val_total\n",
    "#     best_val_acc = max(best_val_acc, val_acc)\n",
    "    \n",
    "#     results.append({\n",
    "#         'lr': params['lr'],\n",
    "#         'batch_size': params['batch_size'],\n",
    "#         'best_val_acc': best_val_acc,\n",
    "#     })\n",
    "\n",
    "# for result in results:\n",
    "#     print(f\"LR: {result['lr']}, Batch Size: {result['batch_size']}, Best Val Acc: {result['best_val_acc']}\")\n",
    "\n",
    "# best_params = max(results, key=lambda x: x['best_val_acc'])\n",
    "# print(f\"Best params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) # test 5/0.5 later\n",
    "\n",
    "patience = 5\n",
    "best_val_acc = 0  \n",
    "patience_counter = 0\n",
    "\n",
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40: 100%|██████████| 1484/1484 [02:35<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.6764746158913462, Train Accuracy: 0.2929828995029905, Test Loss: 1.638372660965048, Test Accuracy: 0.2984020185029437, Validation Loss: 1.9568754089506049, Validation Accuracy: 0.1669449081803005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40: 100%|██████████| 1484/1484 [02:32<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 1.6046063568232194, Train Accuracy: 0.30978856035717295, Test Loss: 1.5499122117155342, Test Accuracy: 0.3633305298570227, Validation Loss: 1.887771531155235, Validation Accuracy: 0.2003338898163606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40: 100%|██████████| 1484/1484 [02:33<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 1.4803356972505461, Train Accuracy: 0.37103024176564736, Test Loss: 1.4157705101915585, Test Accuracy: 0.3860386879730866, Validation Loss: 1.8681549806343882, Validation Accuracy: 0.20534223706176963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40: 100%|██████████| 1484/1484 [02:30<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 1.3996084919676306, Train Accuracy: 0.40826383623957546, Test Loss: 1.403552266859239, Test Accuracy: 0.4529857022708158, Validation Loss: 1.9267147465756065, Validation Accuracy: 0.23205342237061768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40: 100%|██████████| 1484/1484 [02:30<00:00,  9.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 1.3359975114344265, Train Accuracy: 0.4489933451267795, Test Loss: 1.376068888332254, Test Accuracy: 0.48225399495374266, Validation Loss: 1.863081251320086, Validation Accuracy: 0.2621035058430718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40: 100%|██████████| 1484/1484 [02:31<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 1.2810848969012258, Train Accuracy: 0.48037233594473927, Test Loss: 1.2850116143623989, Test Accuracy: 0.4947014297729184, Validation Loss: 1.8789387188459699, Validation Accuracy: 0.2637729549248748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40: 100%|██████████| 1484/1484 [02:28<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 1.2324535188006585, Train Accuracy: 0.49894701373094097, Test Loss: 1.320918600084961, Test Accuracy: 0.4733389402859546, Validation Loss: 1.8742041462346126, Validation Accuracy: 0.2587646076794658\n",
      "No improvement in validation accuracy for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40: 100%|██████████| 1484/1484 [02:29<00:00,  9.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 1.1633084569015915, Train Accuracy: 0.5214388004380422, Test Loss: 1.215117839395359, Test Accuracy: 0.5108494533221194, Validation Loss: 1.9690275160889876, Validation Accuracy: 0.2687813021702838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40: 100%|██████████| 1484/1484 [02:32<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 1.1105626149559922, Train Accuracy: 0.540603150534917, Test Loss: 1.1798690596735606, Test Accuracy: 0.5266610597140454, Validation Loss: 1.9292799610840647, Validation Accuracy: 0.28046744574290483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40: 100%|██████████| 1484/1484 [02:32<00:00,  9.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 1.0480623638453188, Train Accuracy: 0.5678123157274029, Test Loss: 1.1119768483343946, Test Accuracy: 0.5475189234650967, Validation Loss: 1.911868336953615, Validation Accuracy: 0.27712854757929883\n",
      "No improvement in validation accuracy for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40: 100%|██████████| 1484/1484 [02:30<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train Loss: 0.9640586330402251, Train Accuracy: 0.61679723696403, Test Loss: 1.113326737236592, Test Accuracy: 0.5584524810765349, Validation Loss: 1.8183641998391402, Validation Accuracy: 0.32554257095158595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40: 100%|██████████| 1484/1484 [02:31<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train Loss: 0.8460598070666475, Train Accuracy: 0.6681408474433493, Test Loss: 1.021128286307137, Test Accuracy: 0.6179983179142137, Validation Loss: 2.162320466418015, Validation Accuracy: 0.35392320534223703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40: 100%|██████████| 1484/1484 [02:31<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Train Loss: 0.7467343256761122, Train Accuracy: 0.7117765984331564, Test Loss: 0.9746078418227293, Test Accuracy: 0.6270815811606392, Validation Loss: 2.001283259768235, Validation Accuracy: 0.34390651085141904\n",
      "No improvement in validation accuracy for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40: 100%|██████████| 1484/1484 [02:30<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Train Loss: 0.6363311146067159, Train Accuracy: 0.7597085334007244, Test Loss: 1.0340831765043799, Test Accuracy: 0.6410428931875526, Validation Loss: 2.2191431961561503, Validation Accuracy: 0.38731218697829717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40: 100%|██████████| 1484/1484 [02:30<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Train Loss: 0.5181880636175848, Train Accuracy: 0.8107573077247072, Test Loss: 1.0879225365577205, Test Accuracy: 0.6496215306980656, Validation Loss: 3.1089574349553963, Validation Accuracy: 0.3572621035058431\n",
      "No improvement in validation accuracy for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40: 100%|██████████| 1484/1484 [02:31<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Train Loss: 0.40700626868573886, Train Accuracy: 0.8522449667256339, Test Loss: 1.1549632567111203, Test Accuracy: 0.6427249789739277, Validation Loss: 2.483827741522538, Validation Accuracy: 0.41903171953255425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40: 100%|██████████| 1484/1484 [02:30<00:00,  9.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Train Loss: 0.3152365991613524, Train Accuracy: 0.892047847696066, Test Loss: 1.32560329616911, Test Accuracy: 0.6464255677039529, Validation Loss: 3.3758337121260795, Validation Accuracy: 0.41569282136894825\n",
      "No improvement in validation accuracy for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40: 100%|██████████| 1484/1484 [02:30<00:00,  9.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Train Loss: 0.2500251085303518, Train Accuracy: 0.9147923511077416, Test Loss: 1.4694513156309084, Test Accuracy: 0.6467619848612279, Validation Loss: 3.5725974597428976, Validation Accuracy: 0.38731218697829717\n",
      "No improvement in validation accuracy for 2 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40: 100%|██████████| 1484/1484 [02:31<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Train Loss: 0.2076466350466003, Train Accuracy: 0.9274281863364502, Test Loss: 1.4792202414484115, Test Accuracy: 0.6550042052144659, Validation Loss: 3.7953428845656547, Validation Accuracy: 0.39565943238731216\n",
      "No improvement in validation accuracy for 3 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40: 100%|██████████| 1484/1484 [02:31<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Train Loss: 0.17394087313579626, Train Accuracy: 0.9425490691601381, Test Loss: 1.4417778509543828, Test Accuracy: 0.6499579478553407, Validation Loss: 4.143726210845144, Validation Accuracy: 0.3722871452420701\n",
      "No improvement in validation accuracy for 4 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40: 100%|██████████| 1484/1484 [02:32<00:00,  9.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Train Loss: 0.14652970975758808, Train Accuracy: 0.9529104540476793, Test Loss: 1.6551270366756505, Test Accuracy: 0.6442388561816653, Validation Loss: 4.097508518319381, Validation Accuracy: 0.3823038397328882\n",
      "No improvement in validation accuracy for 5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40: 100%|██████████| 1484/1484 [02:31<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Train Loss: 0.13003434041419523, Train Accuracy: 0.960449835734142, Test Loss: 1.7686592559192469, Test Accuracy: 0.6492851135407905, Validation Loss: 4.623237892201073, Validation Accuracy: 0.38397328881469117\n",
      "No improvement in validation accuracy for 6 epochs.\n",
      "Stopping early due to lack of improvement in validation accuracy.\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in tqdm(data_train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(data_train_loader)\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in data_test_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = test_running_loss / len(data_test_loader)\n",
    "    test_acc = test_correct / test_total\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in data_vali_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_running_loss / len(data_vali_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}, Train Accuracy: {train_acc}, Test Loss: {test_loss}, Test Accuracy: {test_acc}, Validation Loss: {val_loss}, Validation Accuracy: {val_acc}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0 \n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in validation accuracy for {patience_counter} epochs.\")\n",
    "    \n",
    "    if patience_counter > patience:\n",
    "        print(\"Stopping early due to lack of improvement in validation accuracy.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (21,) and (22,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m22\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrain Loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# change this number after '(1, _)' to num_epochs+1\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m22\u001b[39m), test_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# change this number after '(1, _)' to num_epochs+1\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m22\u001b[39m), val_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# change this number after '(1, _)' to num_epochs+1\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/pyplot.py:2740\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2738\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   2739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2741\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2742\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/axes/_axes.py:1662\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1662\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1663\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1664\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (21,) and (22,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, 23), train_losses, label='Train Loss') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.plot(range(1, 23), test_losses, label='Test Loss') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.plot(range(1, 23), val_losses, label='Validation Loss') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Losses on RAF-DB') # change\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 23), train_accuracies, label='Train Accuracy') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.plot(range(1, 23), test_accuracies, label='Test Accuracy') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.plot(range(1, 23), val_accuracies, label='Validation Accuracy') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracies on RAF-DB') # change\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Epoch': range(1, 23), # change this number after '(1, _)' to num_epochs+1\n",
    "    'Train Loss': train_losses,\n",
    "    'Test Loss': test_losses,\n",
    "    'Validation Loss': val_losses,\n",
    "    'Train Accuracy': train_accuracies,\n",
    "    'Test Accuracy': test_accuracies,\n",
    "    'Validation Accuracy': val_accuracies\n",
    "})\n",
    "df.to_csv('result_RAF_6FER.csv', index=False) # change this CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
