{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, Sequential, Module\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class RAFDBDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.labels = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.img_dir, self.labels.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        label = self.labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch: image shape torch.Size([16, 3, 64, 64]), labels shape torch.Size([16])\n",
      "Vali batch: image shape torch.Size([16, 3, 64, 64]), labels shape torch.Size([16])\n",
      "Test batch: image shape torch.Size([16, 3, 64, 64]), labels shape torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "from rafdb_dataset import RAFDBDataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # transforms.RandomErasing(scale=(0.02,0.25)),\n",
    "])\n",
    "    \n",
    "rafdb_dataset_train = RAFDBDataset(csv_file='data/train_labels.csv',\n",
    "                            img_dir='data/train/',\n",
    "                            transform=transform)\n",
    "data_train_loader = DataLoader(rafdb_dataset_train, batch_size=16, shuffle=True, num_workers=4)\n",
    "train_image, train_label = next(iter(data_train_loader))\n",
    "print(f\"Train batch: image shape {train_image.shape}, labels shape {train_label.shape}\")\n",
    "\n",
    "rafdb_dataset_vali = RAFDBDataset(csv_file='data/valid_labels.csv',\n",
    "                            img_dir='data/valid',\n",
    "                            transform=transform)\n",
    "data_vali_loader = DataLoader(rafdb_dataset_vali, batch_size=16, shuffle=False, num_workers=0)\n",
    "vali_image, vali_label = next(iter(data_vali_loader))\n",
    "print(f\"Vali batch: image shape {vali_image.shape}, labels shape {vali_label.shape}\")\n",
    "\n",
    "rafdb_dataset_test = RAFDBDataset(csv_file='data/test_labels.csv',\n",
    "                            img_dir='data/test/',\n",
    "                            transform=transform)\n",
    "data_test_loader = DataLoader(rafdb_dataset_test, batch_size=16, shuffle=False, num_workers=0)\n",
    "test_image, test_label = next(iter(data_test_loader))\n",
    "print(f\"Test batch: image shape {test_image.shape}, labels shape {test_label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module): # Squeeze-and-Excitation (SE) blocks apply channel-wise attention.\n",
    "    def __init__(self, input_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_channels, input_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(input_channels // reduction, input_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)  \n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se1 = SEBlock(64)\n",
    "\n",
    "        self.res_block1 = ResidualBlock(64, 128, stride=2)\n",
    "        self.res_block2 = ResidualBlock(128, 256, stride=2)\n",
    "        self.res_block3 = ResidualBlock(256, 512, stride=2)\n",
    "        self.res_block4 = ResidualBlock(512, 1024, stride=2)\n",
    "        self.res_block5 = ResidualBlock(1024, 2048, stride=2)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(2048, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 2048) \n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(2048, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.se1(x)\n",
    "        \n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        x = self.res_block4(x)\n",
    "        x = self.res_block5(x)\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = EmotionClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EmotionClassifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(EmotionClassifier, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "#         self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm2d(128)\n",
    "#         self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "#         self.bn3 = nn.BatchNorm2d(256)\n",
    "#         self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "#         self.bn4 = nn.BatchNorm2d(512)\n",
    "#         self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "#         self.bn5 = nn.BatchNorm2d(1024)\n",
    "\n",
    "#         self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.fc1 = nn.Linear(1024, 2048)\n",
    "#         self.fc2 = nn.Linear(2048, 1024) \n",
    "#         self.dropout1 = nn.Dropout(0.2)\n",
    "#         self.dropout2 = nn.Dropout(0.5)\n",
    "#         self.fc3 = nn.Linear(1024, 6)\n",
    "\n",
    "#     def forward(self, x): # (batch_size, channels=3, 64, 64)\n",
    "#         x = F.relu(self.bn1(self.conv1(x))) # (batch_size, 64, 64, 64)\n",
    "#         x = F.max_pool2d(x, 2) # (batch_size, 64, 32, 32)\n",
    "#         # x = self.dropout1(x)\n",
    "#         x = F.relu(self.bn2(self.conv2(x))) # (batch_size, 128, 32, 32)\n",
    "#         x = F.max_pool2d(x, 2) # (batch_size, 128, 16, 16)\n",
    "#         x = self.dropout1(x)\n",
    "#         x = F.relu(self.bn3(self.conv3(x))) # (batch_size, 256, 16, 16)\n",
    "#         x = F.max_pool2d(x, 2) # (batch_size, 256, 8, 8)\n",
    "#         # x = self.dropout1(x)\n",
    "#         x = F.relu(self.bn4(self.conv4(x))) # (batch_size, 512, 8, 8)\n",
    "#         x = F.max_pool2d(x, 2) # (batch_size, 512, 4, 4)\n",
    "#         x = self.dropout1(x)\n",
    "#         x = F.relu(self.bn5(self.conv5(x))) # (batch_size, 1024, 4, 4)\n",
    "#         x = F.max_pool2d(x, 2) # (batch_size, 1024, 2, 2)\n",
    "#         # x = self.dropout1(x)\n",
    "        \n",
    "#         x = self.pool(x) # (batch_size, 1024, 1, 1)\n",
    "#         x = x.view(x.size(0), -1) # (batch_size, 1024) # Flatten\n",
    "#         x = F.relu(self.fc1(x)) # (batch_size, 2048)\n",
    "#         x = self.dropout2(x) # (batch_size, 2048)\n",
    "#         x = F.relu(self.fc2(x)) # (batch_size, 1024)\n",
    "#         x = self.fc3(x) # (batch_size, 6)\n",
    "#         return x\n",
    "\n",
    "# model = EmotionClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('best_baseline.pth', map_location=device))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 95051014\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class BasicBlock(nn.Module):\n",
    "#     expansion = 1\n",
    "\n",
    "#     def __init__(self, in_planes, planes, stride=1):\n",
    "#         super(BasicBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(planes)\n",
    "#         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "#         self.shortcut = nn.Sequential()\n",
    "#         if stride != 1 or in_planes != self.expansion * planes:\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(self.expansion * planes)\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.bn1(self.conv1(x)))\n",
    "#         out = self.bn2(self.conv2(out))\n",
    "#         out += self.shortcut(x)\n",
    "#         out = F.relu(out)\n",
    "#         return out\n",
    "\n",
    "# class ResNet(nn.Module):\n",
    "#     def __init__(self, block, num_blocks, num_classes=6):\n",
    "#         super(ResNet, self).__init__()\n",
    "#         self.in_planes = 64\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "#         self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "#         self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "#         self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "#         self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "#     def _make_layer(self, block, planes, num_blocks, stride):\n",
    "#         strides = [stride] + [1]*(num_blocks-1)\n",
    "#         layers = []\n",
    "#         for stride in strides:\n",
    "#             layers.append(block(self.in_planes, planes, stride))\n",
    "#             self.in_planes = planes * block.expansion\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.bn1(self.conv1(x)))\n",
    "#         out = self.layer1(out)\n",
    "#         out = self.layer2(out)\n",
    "#         out = self.layer3(out)\n",
    "#         out = self.layer4(out)\n",
    "#         out = self.avgpool(out)\n",
    "#         out = out.view(out.size(0), -1)\n",
    "#         out = self.fc(out)\n",
    "#         return out\n",
    "\n",
    "# def EmotionClassifierResNet18():\n",
    "#     return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "# model = EmotionClassifierResNet18().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class VGGEmotionClassifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(VGGEmotionClassifier, self).__init__()\n",
    "        \n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "#             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "#             nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#         )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(16384, 4096), \n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(4096, 1024),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(1024, 6)  \n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "# model = VGGEmotionClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'lr': [0.1, 0.01, 0.001, 0.0001], \n",
    "#     'batch_size': [8, 16, 32, 64],  \n",
    "# }\n",
    "# grid = ParameterGrid(param_grid)\n",
    "# results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for params in grid: # Hyperparameter tuning \n",
    "#     data_train_loader = DataLoader(rafdb_dataset_train, batch_size=params['batch_size'], shuffle=True, num_workers=4)\n",
    "#     data_vali_loader = DataLoader(rafdb_dataset_vali, batch_size=params['batch_size'], shuffle=False, num_workers=0)\n",
    "    \n",
    "#     model = EmotionClassifier().to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "#     best_val_acc = 0\n",
    "#     num_epochs = 15\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         for i, data in enumerate(tqdm(data_train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"), 0):\n",
    "#             inputs, labels = data[0].to(device), data[1].to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#     model.eval()\n",
    "#     val_correct = 0\n",
    "#     val_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data in data_vali_loader:\n",
    "#             inputs, labels = data[0].to(device), data[1].to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             val_total += labels.size(0)\n",
    "#             val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "#     val_acc = val_correct / val_total\n",
    "#     best_val_acc = max(best_val_acc, val_acc)\n",
    "    \n",
    "#     results.append({\n",
    "#         'lr': params['lr'],\n",
    "#         'batch_size': params['batch_size'],\n",
    "#         'best_val_acc': best_val_acc,\n",
    "#     })\n",
    "\n",
    "# for result in results:\n",
    "#     print(f\"LR: {result['lr']}, Batch Size: {result['batch_size']}, Best Val Acc: {result['best_val_acc']}\")\n",
    "\n",
    "# best_params = max(results, key=lambda x: x['best_val_acc'])\n",
    "# print(f\"Best params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001, amsgrad=True)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "patience = 10\n",
    "best_val_acc = 0  \n",
    "patience_counter = 0\n",
    "\n",
    "num_epochs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/80: 100%|██████████| 3442/3442 [11:42<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.2810049423691425, Train Accuracy: 0.48777033284305715, Test Loss: 1.0847956004504278, Test Accuracy: 0.5902544407105137, Validation Loss: 1.2896715653570074, Validation Accuracy: 0.5225375626043406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/80: 100%|██████████| 3442/3442 [11:27<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 1.0548258203794483, Train Accuracy: 0.5846089593433931, Test Loss: 0.9770676386104664, Test Accuracy: 0.6183389342294767, Validation Loss: 1.2903187510214353, Validation Accuracy: 0.5342237061769616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/80: 100%|██████████| 3442/3442 [11:28<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.9447331909676889, Train Accuracy: 0.6340723792921864, Test Loss: 0.947404752762527, Test Accuracy: 0.6290206433029285, Validation Loss: 1.1101203403974835, Validation Accuracy: 0.5959933222036727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/80: 100%|██████████| 3442/3442 [11:30<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.8688267148957566, Train Accuracy: 0.6634526338726371, Test Loss: 0.8733703281851968, Test Accuracy: 0.670907345175228, Validation Loss: 0.9679908321092003, Validation Accuracy: 0.6494156928213689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/80: 100%|██████████| 3442/3442 [11:30<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.8045127548532802, Train Accuracy: 0.6906720415463674, Test Loss: 0.879145609553646, Test Accuracy: 0.6689870379260682, Validation Loss: 1.0406629709820998, Validation Accuracy: 0.6377295492487479\n",
      "No improvement in validation accuracy for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/80: 100%|██████████| 3442/3442 [11:28<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.7481012073394727, Train Accuracy: 0.7137876559350657, Test Loss: 0.8635230733603668, Test Accuracy: 0.6790686509841575, Validation Loss: 0.8975565362917749, Validation Accuracy: 0.6978297161936561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/80: 100%|██████████| 3442/3442 [11:26<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.6938102978050605, Train Accuracy: 0.7350329574549218, Test Loss: 0.8208870986555193, Test Accuracy: 0.6880700912145943, Validation Loss: 0.9031172653562144, Validation Accuracy: 0.671118530884808\n",
      "No improvement in validation accuracy for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/80: 100%|██████████| 3442/3442 [11:27<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.6415306739162768, Train Accuracy: 0.7564235259937172, Test Loss: 0.8248519561827297, Test Accuracy: 0.7034325492078732, Validation Loss: 0.9933646791859677, Validation Accuracy: 0.666110183639399\n",
      "No improvement in validation accuracy for 2 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/80: 100%|██████████| 3442/3442 [11:20<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.5901672279658116, Train Accuracy: 0.7775417188720016, Test Loss: 0.8613952355934825, Test Accuracy: 0.7005520883341335, Validation Loss: 1.0608880872789181, Validation Accuracy: 0.669449081803005\n",
      "No improvement in validation accuracy for 3 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/80: 100%|██████████| 3442/3442 [11:21<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.5414806370174795, Train Accuracy: 0.7983330609576729, Test Loss: 0.8714037549695591, Test Accuracy: 0.6929908785405665, Validation Loss: 1.0049533000902127, Validation Accuracy: 0.6978297161936561\n",
      "No improvement in validation accuracy for 4 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/80: 100%|██████████| 3442/3442 [11:25<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train Loss: 0.4850130573127209, Train Accuracy: 0.8212126164405948, Test Loss: 0.8580756163181676, Test Accuracy: 0.7043927028324531, Validation Loss: 1.0474769418176852, Validation Accuracy: 0.6861435726210351\n",
      "No improvement in validation accuracy for 5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/80: 100%|██████████| 3442/3442 [11:25<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train Loss: 0.43861452712342447, Train Accuracy: 0.8381362241470103, Test Loss: 0.9207259316813274, Test Accuracy: 0.6955112818050888, Validation Loss: 0.9843056974442381, Validation Accuracy: 0.7128547579298832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/80: 100%|██████████| 3442/3442 [11:17<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Train Loss: 0.39550914109956725, Train Accuracy: 0.8550235150986908, Test Loss: 0.9552355540510545, Test Accuracy: 0.7021123379740759, Validation Loss: 1.0278917652995962, Validation Accuracy: 0.7061769616026711\n",
      "No improvement in validation accuracy for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/80: 100%|██████████| 3442/3442 [10:52<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Train Loss: 0.3527958558878836, Train Accuracy: 0.8732545259755589, Test Loss: 1.011121958727464, Test Accuracy: 0.6985117618819011, Validation Loss: 1.0087207195005918, Validation Accuracy: 0.7111853088480802\n",
      "No improvement in validation accuracy for 2 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/80: 100%|██████████| 3442/3442 [10:52<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Train Loss: 0.31449857003849385, Train Accuracy: 0.8885801964736432, Test Loss: 1.0360022516643643, Test Accuracy: 0.6861497839654345, Validation Loss: 0.9466624354061327, Validation Accuracy: 0.7445742904841403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/80: 100%|██████████| 3442/3442 [10:52<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Train Loss: 0.28888608865442056, Train Accuracy: 0.898422037006773, Test Loss: 1.0523502039239616, Test Accuracy: 0.6950312049927988, Validation Loss: 1.1731349939578457, Validation Accuracy: 0.6994991652754591\n",
      "No improvement in validation accuracy for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/80: 100%|██████████| 3442/3442 [10:52<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Train Loss: 0.2536034656028095, Train Accuracy: 0.9117684443718109, Test Loss: 1.0807253784938464, Test Accuracy: 0.7016322611617859, Validation Loss: 1.1543956191131943, Validation Accuracy: 0.7078464106844741\n",
      "No improvement in validation accuracy for 2 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/80: 100%|██████████| 3442/3442 [10:52<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Train Loss: 0.22993021238327918, Train Accuracy: 0.9203210401118556, Test Loss: 1.1284169576581273, Test Accuracy: 0.6980316850696111, Validation Loss: 1.1287556078873182, Validation Accuracy: 0.7245409015025042\n",
      "No improvement in validation accuracy for 3 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/80: 100%|██████████| 3442/3442 [10:52<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Train Loss: 0.21096655825450086, Train Accuracy: 0.9273301737756714, Test Loss: 1.1468015817989352, Test Accuracy: 0.7022323571771484, Validation Loss: 1.3197346839465594, Validation Accuracy: 0.7078464106844741\n",
      "No improvement in validation accuracy for 4 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/80: 100%|██████████| 3442/3442 [10:51<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Train Loss: 0.19767296288910208, Train Accuracy: 0.9317426594759493, Test Loss: 1.1755252098651234, Test Accuracy: 0.6958713394143062, Validation Loss: 1.2755893162990872, Validation Accuracy: 0.7228714524207012\n",
      "No improvement in validation accuracy for 5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/80: 100%|██████████| 3442/3442 [10:51<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Train Loss: 0.1812285120463819, Train Accuracy: 0.9382252001961104, Test Loss: 1.2405583266563047, Test Accuracy: 0.6951512241958714, Validation Loss: 1.3390442552535158, Validation Accuracy: 0.7061769616026711\n",
      "No improvement in validation accuracy for 6 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/80: 100%|██████████| 3442/3442 [10:51<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Train Loss: 0.17037518134799676, Train Accuracy: 0.9423653102358773, Test Loss: 1.225006084152612, Test Accuracy: 0.7012722035525684, Validation Loss: 1.2746786620271833, Validation Accuracy: 0.7262103505843072\n",
      "No improvement in validation accuracy for 7 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/80: 100%|██████████| 3442/3442 [10:51<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Train Loss: 0.16153765859905975, Train Accuracy: 0.9451072252183544, Test Loss: 1.2541156560430597, Test Accuracy: 0.6975516082573212, Validation Loss: 1.2126620654997073, Validation Accuracy: 0.7128547579298832\n",
      "No improvement in validation accuracy for 8 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/80: 100%|██████████| 3442/3442 [10:51<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Train Loss: 0.15631474125662245, Train Accuracy: 0.9464691035209094, Test Loss: 1.3013401033745917, Test Accuracy: 0.6910705712914066, Validation Loss: 1.3379830200421183, Validation Accuracy: 0.7095158597662772\n",
      "No improvement in validation accuracy for 9 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/80: 100%|██████████| 3442/3442 [10:52<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Train Loss: 0.14811597560738163, Train Accuracy: 0.9489931179749778, Test Loss: 1.2197027208257625, Test Accuracy: 0.6943110897743638, Validation Loss: 1.2845733106920594, Validation Accuracy: 0.7128547579298832\n",
      "No improvement in validation accuracy for 10 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/80: 100%|██████████| 3442/3442 [10:52<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Train Loss: 0.1424138122215558, Train Accuracy: 0.9520618837500681, Test Loss: 1.2557476735649544, Test Accuracy: 0.6976716274603937, Validation Loss: 1.2985445813913095, Validation Accuracy: 0.7145242070116862\n",
      "No improvement in validation accuracy for 11 epochs.\n",
      "Stopping early due to lack of improvement in validation accuracy.\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in tqdm(data_train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(data_train_loader)\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in data_test_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = test_running_loss / len(data_test_loader)\n",
    "    test_acc = test_correct / test_total\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in data_vali_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_running_loss / len(data_vali_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}, Train Accuracy: {train_acc}, Test Loss: {test_loss}, Test Accuracy: {test_acc}, Validation Loss: {val_loss}, Validation Accuracy: {val_acc}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0 \n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in validation accuracy for {patience_counter} epochs.\")\n",
    "    \n",
    "    if patience_counter > patience:\n",
    "        print(\"Stopping early due to lack of improvement in validation accuracy.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (44,) and (26,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m45\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrain Loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# change this number after '(1, _)' to num_epochs+1\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m45\u001b[39m), test_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# change this number after '(1, _)' to num_epochs+1\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m45\u001b[39m), val_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# change this number after '(1, _)' to num_epochs+1\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/pyplot.py:3575\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3567\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3569\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3573\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3574\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3576\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/axes/_axes.py:1721\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1721\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/axes/_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    302\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/axes/_base.py:499\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    496\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (44,) and (26,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, 45), train_losses, label='Train Loss') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.plot(range(1, 45), test_losses, label='Test Loss') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.plot(range(1, 45), val_losses, label='Validation Loss') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Losses on GiMeFive') # change\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 45), train_accuracies, label='Train Accuracy') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.plot(range(1, 45), test_accuracies, label='Test Accuracy') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.plot(range(1, 45), val_accuracies, label='Validation Accuracy') # change this number after '(1, _)' to num_epochs+1\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracies on GiMeFive') # change\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Epoch': range(1, 45), # change this number after '(1, _)' to num_epochs+1\n",
    "    'Train Loss': train_losses,\n",
    "    'Test Loss': test_losses,\n",
    "    'Validation Loss': val_losses,\n",
    "    'Train Accuracy': train_accuracies,\n",
    "    'Test Accuracy': test_accuracies,\n",
    "    'Validation Accuracy': val_accuracies\n",
    "})\n",
    "df.to_csv('result_ec2DO.csv', index=False) # change this CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
