%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Leah Kawka at 2024-01-15 20:42:26 +0100 


%% Saved with string encoding Unicode (UTF-8) 


@string{aaai = {AAAI}}

@string{accv = {ACCV}}

@string{acmmm = {ACM MM}}

@string{bmvc = {BMVC}}

@string{cgf = {Comput. Graph. Forum}}

@string{csvt = {IEEE TCSVT}}

@string{cvm = {Computational Visual Media}}

@string{cvpr = {CVPR}}

@string{cvprw = {CVPRW}}

@string{eccv = {ECCV}}

@string{icassp = {ICASSP}}

@string{iccv = {ICCV}}

@string{icip = {ICIP}}

@string{iclr = {ICLR}}

@string{icme = {ICME}}

@string{icpr = {ICPR}}

@string{ijcai = {IJCAI}}

@string{ijcv = {IJCV}}

@string{jcst = {J. Comput. Sci. Tech.}}

@string{jov = {J. Vis.}}

@string{nips = {NeurIPS}}

@string{pami = {IEEE TPAMI}}

@string{pr = {PR}}

@string{spl = {IEEE Sign. Process. Letters}}

@string{tcsvt = {IEEE TCSVT}}

@string{tip = {IEEE TIP}}

@string{tmm = {IEEE TMM}}

@string{tog = {ACM TOG}}

@string{tvc = {The Vis. Comput.}}

@string{tvcg = {IEEE TVCG}}

@string{vr = {Vis. Res.}}


@article{li2019reliable,
	author = {Li, Shan and Deng, Weihong},
	journal = {IEEE Transactions on Image Processing},
	number = {1},
	pages = {356--370},
	publisher = {IEEE},
	title = {Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition},
	volume = {28},
	year = {2019}}

@inproceedings{LuceyCKSAM10,
	author = {Patrick Lucey and Jeffrey F. Cohn and Takeo Kanade and Jason M. Saragih and Zara Ambadar and Iain A. Matthews},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/cvpr/LuceyCKSAM10.bib},
	booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} Workshops 2010, San Francisco, CA, USA, 13-18 June, 2010},
	doi = {10.1109/CVPRW.2010.5543262},
	pages = {94--101},
	publisher = {{IEEE} Computer Society},
	timestamp = {Fri, 24 Mar 2023 00:02:54 +0100},
	title = {The Extended Cohn-Kanade Dataset {(CK+):} {A} complete dataset for action unit and emotion-specified expression},
	url = {https://doi.org/10.1109/CVPRW.2010.5543262},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/CVPRW.2010.5543262}}

@inproceedings{BarsoumZCZ16,
	author = {Emad Barsoum and Cha Zhang and Cristian Canton{-}Ferrer and Zhengyou Zhang},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/icmi/BarsoumZCZ16.bib},
	booktitle = {Proceedings of the 18th {ACM} International Conference on Multimodal Interaction, {ICMI} 2016, Tokyo, Japan, November 12-16, 2016},
	doi = {10.1145/2993148.2993165},
	editor = {Yukiko I. Nakano and Elisabeth Andr{\'{e}} and Toyoaki Nishida and Louis{-}Philippe Morency and Carlos Busso and Catherine Pelachaud},
	pages = {279--283},
	publisher = {{ACM}},
	timestamp = {Sat, 30 Sep 2023 09:45:54 +0200},
	title = {Training deep networks for facial expression recognition with crowd-sourced label distribution},
	url = {https://doi.org/10.1145/2993148.2993165},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1145/2993148.2993165}}

@article{Ko18,
	author = {ByoungChul Ko},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/sensors/Ko18.bib},
	doi = {10.3390/S18020401},
	journal = {Sensors},
	number = {2},
	pages = {401},
	timestamp = {Wed, 14 Nov 2018 10:46:37 +0100},
	title = {A Brief Review of Facial Emotion Recognition Based on Visual Information},
	url = {https://doi.org/10.3390/s18020401},
	volume = {18},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3390/s18020401}}

@inproceedings{HeZRS16,
	author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/cvpr/HeZRS16.bib},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
	doi = {10.1109/CVPR.2016.90},
	pages = {770--778},
	publisher = {{IEEE} Computer Society},
	timestamp = {Fri, 24 Mar 2023 00:02:57 +0100},
	title = {Deep Residual Learning for Image Recognition},
	url = {https://doi.org/10.1109/CVPR.2016.90},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/CVPR.2016.90}}

@misc{verma_efficient_2023,
	abstract = {RAF-DB, and 6 micro-expression datasets: CASME-I, CASME-II, CAS(ME)ˆ2, SAMM, SMIC, MEGC2019 challenge). The proposed models outperform the existing state-of-the-art methods and perform very well in terms of speed and space complexity.},
	author = {Verma, Monu and Mandal, Murari and Reddy, Satish Kumar and Meedimale, Yashwanth Reddy and Vipparthi, Santosh Kumar},
	file = {Verma et al. - 2023 - Efficient Neural Architecture Search for Emotion R.pdf:/Users/kawka/Zotero/storage/S56ACBIM/Verma et al. - 2023 - Efficient Neural Architecture Search for Emotion R.pdf:application/pdf},
	language = {en},
	month = mar,
	note = {arXiv:2303.13653 [cs]},
	publisher = {arXiv},
	title = {Efficient {Neural} {Architecture} {Search} for {Emotion} {Recognition}},
	url = {http://arxiv.org/abs/2303.13653},
	urldate = {2024-01-01},
	year = {2023},
	bdsk-url-1 = {http://arxiv.org/abs/2303.13653}}

@inproceedings{li_reliable_2017,
	abstract = {Past research on facial expressions have used relatively limited datasets, which makes it unclear whether current methods can be employed in real world. In this paper, we present a novel database, RAF-DB, which contains about 30000 facial images from thousands of individuals. Each image has been individually labeled about 40 times, then EM algorithm was used to filter out unreliable labels. Crowdsourcing reveals that real-world faces often express compound emotions, or even mixture ones. For all we know, RAF-DB is the first database that contains compound expressions in the wild. Our cross-database study shows that the action units of basic emotions in RAF-DB are much more diverse than, or even deviate from, those of labcontrolled ones. To address this problem, we propose a new DLP-CNN (Deep Locality-Preserving CNN) method, which aims to enhance the discriminative power of deep features by preserving the locality closeness while maximizing the inter-class scatters. The benchmark experiments on the 7class basic expressions and 11-class compound expressions, as well as the additional experiments on SFEW and CK+ databases, show that the proposed DLP-CNN outperforms the state-of-the-art handcrafted features and deep learning based methods for the expression recognition in the wild.},
	address = {Honolulu, HI},
	author = {Li, Shan and Deng, Weihong and Du, JunPing},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	doi = {10.1109/CVPR.2017.277},
	file = {Li et al. - 2017 - Reliable Crowdsourcing and Deep Locality-Preservin.pdf:/Users/kawka/Zotero/storage/B5CZBGZC/Li et al. - 2017 - Reliable Crowdsourcing and Deep Locality-Preservin.pdf:application/pdf},
	isbn = {978-1-5386-0457-1},
	language = {en},
	month = jul,
	pages = {2584--2593},
	publisher = {IEEE},
	title = {Reliable {Crowdsourcing} and {Deep} {Locality}-{Preserving} {Learning} for {Expression} {Recognition} in the {Wild}},
	url = {http://ieeexplore.ieee.org/document/8099760/},
	urldate = {2024-01-01},
	year = {2017},
	bdsk-url-1 = {http://ieeexplore.ieee.org/document/8099760/},
	bdsk-url-2 = {https://doi.org/10.1109/CVPR.2017.277}}

@article{baldeon_calisto_emonas-net_2021,
	abstract = {Deep learning plays a critical role in medical image segmentation. Nevertheless, manually designing a neural network for a specific segmentation problem is a very difficult and time-consuming task due to the massive hyperparameter search space, long training time and large volumetric data. Therefore, most designed networks are highly complex, task specific and over-parametrized. Recently, multiobjective neural architecture search (NAS) methods have been proposed to automate the design of accurate and efficient segmentation architectures. However, they only search for either the micro- or macro-structure of the architecture, do not use the information produced during the optimization process to increase the efficiency of the search, or do not consider the volumetric nature of medical images. In this work, we present EMONAS-Net, an Efficient MultiObjective NAS framework for 3D medical image segmentation that optimizes both the segmentation accuracy and size of the network. EMONAS-Net has two key components, a novel search space that considers the configuration of the micro- and macro-structure of the architecture and a Surrogate-assisted Multiobjective Evolutionary based Algorithm (SaMEA algorithm) that efficiently searches for the best hyperparameter values. The SaMEA algorithm uses the information collected during the initial generations of the evolutionary process to identify the most promising subproblems and select the best performing hyperparameter values during mutation to improve the convergence speed. Furthermore, a Random Forest surrogate model is incorporated to accelerate the fitness evaluation of the candidate architectures. EMONAS-Net is tested on the tasks of prostate segmentation from the MICCAI PROMISE12 challenge, hippocampus segmentation from the Medical Segmentation Decathlon challenge, and cardiac segmentation from the MICCAI ACDC challenge. In all the benchmarks, the proposed framework finds architectures that perform better or comparable with competing state-of-the-art NAS methods while being considerably smaller and reducing the architecture search time by more than 50\%.},
	author = {Baldeon Calisto, Maria and Lai-Yuen, Susana K.},
	doi = {10.1016/j.artmed.2021.102154},
	file = {ScienceDirect Snapshot:/Users/kawka/Zotero/storage/K38S7SWV/S0933365721001470.html:text/html},
	issn = {0933-3657},
	journal = {Artificial Intelligence in Medicine},
	month = sep,
	pages = {102154},
	shorttitle = {{EMONAS}-{Net}},
	title = {{EMONAS}-{Net}: {Efficient} multiobjective neural architecture search using surrogate-assisted evolutionary algorithm for {3D} medical image segmentation},
	url = {https://www.sciencedirect.com/science/article/pii/S0933365721001470},
	urldate = {2024-01-01},
	volume = {119},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0933365721001470},
	bdsk-url-2 = {https://doi.org/10.1016/j.artmed.2021.102154}}

@misc{zeiler_visualizing_2013,
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark (Krizhevsky et al., 2012). However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	file = {Zeiler und Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:/Users/kawka/Zotero/storage/RIAFZT4Y/Zeiler und Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf},
	language = {en},
	month = nov,
	note = {arXiv:1311.2901 [cs]},
	publisher = {arXiv},
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1311.2901},
	urldate = {2024-01-01},
	year = {2013},
	bdsk-url-1 = {http://arxiv.org/abs/1311.2901}}

@article{wang_ddam_2023,
	abstract = {Convolution neural networks (CNNs) are widely used algorithms in image processing, natural language processing and many other fields. The large amount of memory access of CNNs is one of the major concerns in CNN accelerator designs that influences the performance and energy-efficiency. With fast and low-cost memory access, Processing-In-Memory (PIM) system is a feasible solution to alleviate the memory concern of CNNs. However, the distributed manner of data storing in PIM systems is in conflict with the large amount of data reuse of CNN layers. Nodes of PIM systems may need to share their data with each other before processing a CNN layer, leading to extra communication overhead. In this article, we propose DDAM to map CNNs onto PIM systems with the communication overhead reduced. Firstly, A data transfer strategy is proposed to deal with the data sharing requirement among PIM nodes by formulating a Traveling-Salesman-Problem (TSP). To improve data locality, a dynamic programming algorithm is proposed to partition the CNN and allocate a number of nodes to each part. Finally, an integer linear programming (ILP)-based mapping algorithm is proposed to map the partitioned CNN onto the PIM system. Experimental results show that compared to the baselines, DDAM can get a higher throughput of 2.0× with the energy cost reduced by 37\% on average.},
	author = {Wang, Junpeng and Du, Haitao and Ding, Bo and Xu, Qi and Chen, Song and Kang, Yi},
	doi = {10.1145/3576196},
	issn = {1084-4309},
	journal = {ACM Trans. Des. Autom. Electron. Syst.},
	month = mar,
	number = {3},
	pages = {36:1--36:30},
	shorttitle = {{DDAM}},
	title = {{DDAM}: {Data} {Distribution}-{Aware} {Mapping} of {CNNs} on {Processing}-{In}-{Memory} {Systems}},
	url = {https://doi.org/10.1145/3576196},
	urldate = {2024-01-01},
	volume = {28},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1145/3576196}}

@misc{noauthor_mug_nodate,
	language = {en-US},
	title = {The {MUG} {Facial} {Expression} {Database} {\textbar} {Multimedia} {Understanding} {Group}},
	url = {https://mug.ee.auth.gr/fed/},
	urldate = {2024-01-01},
	bdsk-url-1 = {https://mug.ee.auth.gr/fed/}}

@misc{noauthor_ck_nodate,
	file = {spenceryee/CS229:/Users/kawka/Zotero/storage/RSEXSDAJ/CS229.html:text/html},
	title = {{CK}+, spenceryee/{CS229}},
	url = {https://github.com/spenceryee/CS229},
	urldate = {2024-01-01},
	bdsk-url-1 = {https://github.com/spenceryee/CS229}}

@misc{noauthor_enduserlicenseagreement_oulu-npupdf_nodate,
	file = {Snapshot:/Users/kawka/Zotero/storage/BIBG7622/view.html:text/html},
	journal = {Google Docs},
	title = {{EndUserLicenseAgreement}\_OULU-{NPU}.pdf},
	url = {https://drive.google.com/file/d/1aJOmWsveeQoLoN7xKHjOr4NzW9g9HP_3/view?usp=sharing&usp=embed_facebook},
	urldate = {2024-01-01},
	bdsk-url-1 = {https://drive.google.com/file/d/1aJOmWsveeQoLoN7xKHjOr4NzW9g9HP_3/view?usp=sharing&usp=embed_facebook}}

@misc{noauthor_real-world_nodate,
	file = {Real-world Affective Faces (RAF) Database:/Users/kawka/Zotero/storage/SIQTAYPA/model1.html:text/html},
	title = {Real-world {Affective} {Faces} ({RAF}) {Database}},
	url = {http://www.whdeng.cn/raf/model1.html},
	urldate = {2024-01-04},
	bdsk-url-1 = {http://www.whdeng.cn/raf/model1.html}}

@misc{noauthor_ck_nodate-1,
	abstract = {Extended Cohn-Kanade dataset},
	file = {Snapshot:/Users/kawka/Zotero/storage/CNDAS426/ck-dataset.html:text/html},
	language = {en},
	title = {{CK}+ dataset},
	url = {https://www.kaggle.com/datasets/shuvoalok/ck-dataset},
	urldate = {2024-01-09},
	bdsk-url-1 = {https://www.kaggle.com/datasets/shuvoalok/ck-dataset}}

@misc{noauthor_microsoftferplus_2024,
	abstract = {This is the FER+ new label annotations for the Emotion FER dataset.},
	month = jan,
	note = {original-date: 2016-09-14T06:35:21Z},
	publisher = {Microsoft},
	title = {microsoft/{FERPlus}},
	url = {https://github.com/microsoft/FERPlus},
	urldate = {2024-01-09},
	year = {2024},
	bdsk-url-1 = {https://github.com/microsoft/FERPlus}}

@misc{noauthor_mmi_nodate,
	file = {MMI Facial Expression Database - Home:/Users/kawka/Zotero/storage/IL29QL5V/mmifacedb.eu.html:text/html},
	title = {{MMI} {Facial} {Expression} {Database} - {Home}},
	url = {https://mmifacedb.eu/},
	urldate = {2024-01-09},
	bdsk-url-1 = {https://mmifacedb.eu/}}

@misc{noauthor_ised_nodate,
	abstract = {Welcome to Indian Spontaneous Expression Database (ISED).
We present a spontaneous expression dataset for the advancement of research in facial expression analysis. Near frontal face video was recorded for 50 participants while watching emotional video clips. The  novel  experiment  design  induced},
	file = {Snapshot:/Users/kawka/Zotero/storage/KL68I2LJ/iseddatabase.html:text/html},
	language = {de},
	title = {{ISED} {Database}},
	url = {https://sites.google.com/site/iseddatabase/},
	urldate = {2024-01-09},
	bdsk-url-1 = {https://sites.google.com/site/iseddatabase/}}

@misc{noauthor_face_nodate,
	abstract = {Kaggle is the world's largest data science community with powerful tools and resources to help you achieve your data science goals.},
	file = {Snapshot:/Users/kawka/Zotero/storage/XZMDS8UF/face-expression-recognition-dataset.html:text/html},
	language = {en},
	title = {Face expression recognition dataset},
	url = {https://www.kaggle.com/datasets/jonathanoheix/face-expression-recognition-dataset},
	urldate = {2024-01-09},
	bdsk-url-1 = {https://www.kaggle.com/datasets/jonathanoheix/face-expression-recognition-dataset}}

@misc{spenceryee_spenceryeecs229_2023,
	author = {spenceryee},
	month = nov,
	note = {original-date: 2014-11-16T06:35:08Z},
	title = {spenceryee/{CS229}},
	url = {https://github.com/spenceryee/CS229},
	urldate = {2024-01-09},
	year = {2023},
	bdsk-url-1 = {https://github.com/spenceryee/CS229}}

@misc{noauthor_raf-db_nodate,
	abstract = {For Recognize emotion from Facial expression},
	file = {Snapshot:/Users/kawka/Zotero/storage/FI279AGQ/code.html:text/html},
	language = {en},
	title = {{RAF}-{DB} {DATASET}},
	url = {https://www.kaggle.com/datasets/shuvoalok/raf-db-dataset},
	urldate = {2024-01-09},
	bdsk-url-1 = {https://www.kaggle.com/datasets/shuvoalok/raf-db-dataset}}

@misc{noauthor_multimodal-emotion-recognition00-presentationreportsp4pdf_nodate,
	file = {Multimodal-Emotion-Recognition/00-Presentation/Reports/P4.pdf at master · maelfabien/Multimodal-Emotion-Recognition:/Users/kawka/Zotero/storage/Q2WSU43U/P4.html:text/html},
	title = {Multimodal-{Emotion}-{Recognition}/00-{Presentation}/{Reports}/{P4}.pdf at master · maelfabien/{Multimodal}-{Emotion}-{Recognition}},
	url = {https://github.com/maelfabien/Multimodal-Emotion-Recognition/blob/master/00-Presentation/Reports/P4.pdf},
	urldate = {2024-01-11},
	bdsk-url-1 = {https://github.com/maelfabien/Multimodal-Emotion-Recognition/blob/master/00-Presentation/Reports/P4.pdf}}

@misc{nagarajan_face_2022,
	abstract = {In this blog, I'm going to explain how to recognize face emotions from images using different Deep Learning models.},
	author = {Nagarajan, Prabhitha},
	file = {Snapshot:/Users/kawka/Zotero/storage/TH9UHU2U/face-emotion-recognition-fer-114ccb359604.html:text/html},
	journal = {MLearning.ai},
	language = {en},
	month = jan,
	title = {Face {Emotion} {Recognition} ({FER})},
	url = {https://medium.com/mlearning-ai/face-emotion-recognition-fer-114ccb359604},
	urldate = {2024-01-13},
	year = {2022},
	bdsk-url-1 = {https://medium.com/mlearning-ai/face-emotion-recognition-fer-114ccb359604}}

@inproceedings{zhou_learning_2016,
	author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
	file = {Full Text PDF:/Users/kawka/Zotero/storage/XWU4VJ84/Zhou et al. - 2016 - Learning Deep Features for Discriminative Localiza.pdf:application/pdf},
	pages = {2921--2929},
	title = {Learning {Deep} {Features} for {Discriminative} {Localization}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Learning_Deep_Features_CVPR_2016_paper.html},
	urldate = {2024-01-15},
	year = {2016},
	bdsk-url-1 = {https://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Learning_Deep_Features_CVPR_2016_paper.html}}

@misc{russakovsky_imagenet_2015,
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	doi = {10.48550/arXiv.1409.0575},
	file = {arXiv Fulltext PDF:/Users/kawka/Zotero/storage/ZTMGUE7A/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:application/pdf;arXiv.org Snapshot:/Users/kawka/Zotero/storage/RSI4YXX9/1409.html:text/html},
	month = jan,
	note = {arXiv:1409.0575 [cs]},
	publisher = {arXiv},
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	url = {http://arxiv.org/abs/1409.0575},
	urldate = {2024-01-15},
	year = {2015},
	bdsk-url-1 = {http://arxiv.org/abs/1409.0575},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.1409.0575}}

@misc{almeida_grad-cam_2023,
	abstract = {Using gradients to understand how your model predicts},
	author = {Almeida, Vin{\'\i}cius},
	file = {Snapshot:/Users/kawka/Zotero/storage/D98FTWKV/grad-cam-in-pytorch-use-of-forward-and-backward-hooks-7eba5e38d569.html:text/html},
	journal = {Medium},
	language = {en},
	month = may,
	shorttitle = {Grad-{CAM} in {Pytorch}},
	title = {Grad-{CAM} in {Pytorch}: {Use} of {Forward} and {Backward} {Hooks}},
	url = {https://towardsdatascience.com/grad-cam-in-pytorch-use-of-forward-and-backward-hooks-7eba5e38d569},
	urldate = {2024-01-15},
	year = {2023},
	bdsk-url-1 = {https://towardsdatascience.com/grad-cam-in-pytorch-use-of-forward-and-backward-hooks-7eba5e38d569}}

@misc{ulyanin_implementing_2019,
	abstract = {Recently I have come across a chapter in Fran{\c c}ois Chollet's ``Deep Learning With Python'' book, describing the implementation of Class{\ldots}},
	author = {Ulyanin, Stepan},
	file = {Snapshot:/Users/kawka/Zotero/storage/VFICHLIA/implementing-grad-cam-in-pytorch-ea0937c31e82.html:text/html},
	journal = {Medium},
	language = {en},
	month = feb,
	title = {Implementing {Grad}-{CAM} in {PyTorch}},
	url = {https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82},
	urldate = {2024-01-15},
	year = {2019},
	bdsk-url-1 = {https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82}}

@misc{noauthor_working_2022,
	abstract = {In this article, we took a look at working with custom datasets in PyTorch to curated a custom dataset via web scraping, load and label it, and created a PyTorch dataset from it.},
	file = {Snapshot:/Users/kawka/Zotero/storage/EBN4S64F/working-with-custom-image-datasets-in-pytorch.html:text/html},
	journal = {Paperspace Blog},
	language = {en},
	month = sep,
	title = {Working with {Custom} {Image} {Datasets} in {PyTorch}},
	url = {https://blog.paperspace.com/working-with-custom-image-datasets-in-pytorch/},
	urldate = {2024-01-15},
	year = {2022},
	bdsk-url-1 = {https://blog.paperspace.com/working-with-custom-image-datasets-in-pytorch/}}

@misc{noauthor_pytorch_nodate,
	abstract = {Learn important machine learning concepts hands-on by writing PyTorch code.},
	file = {Snapshot:/Users/kawka/Zotero/storage/59QWXDGW/04_pytorch_custom_datasets.html:text/html},
	language = {en},
	title = {{PyTorch} {Custom} {Datasets} - {Zero} to {Mastery} {Learn} {PyTorch} for {Deep} {Learning}},
	url = {https://www.learnpytorch.io/04_pytorch_custom_datasets/},
	urldate = {2024-01-15},
	bdsk-url-1 = {https://www.learnpytorch.io/04_pytorch_custom_datasets/}}

@misc{noauthor_disfa_nodate,
	file = {Snapshot:/Users/kawka/Zotero/storage/448TIA9Q/databases-codes.html:text/html},
	language = {en-US},
	title = {{DISFA} {Databases} \& {Codes} -- {Mohammad} {H}. {Mahoor}, {PhD}},
	url = {http://mohammadmahoor.com/databases-codes/},
	urldate = {2024-01-15},
	bdsk-url-1 = {http://mohammadmahoor.com/databases-codes/}}

@INPROCEEDINGS{5543262,
    author={Lucey, Patrick and Cohn, Jeffrey F. and Kanade, Takeo and Saragih, Jason and Ambadar, Zara and Matthews, Iain},
    booktitle={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops}, 
    title={The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression}, 
    year={2010},
    volume={},
    number={},
    pages={94-101},
    doi={10.1109/CVPRW.2010.5543262}}

@misc{barsoum_training_2016,
	title = {Training {Deep} {Networks} for {Facial} {Expression} {Recognition} with {Crowd}-{Sourced} {Label} {Distribution}},
	url = {http://arxiv.org/abs/1608.01041},
	abstract = {Crowd sourcing has become a widely adopted scheme to collect ground truth labels. However, it is a well-known problem that these labels can be very noisy. In this paper, we demonstrate how to learn a deep convolutional neural network (DCNN) from noisy labels, using facial expression recognition as an example. More speciﬁcally, we have 10 taggers to label each input image, and compare four different approaches to utilizing the multiple labels: majority voting, multi-label learning, probabilistic label drawing, and cross-entropy loss. We show that the traditional majority voting scheme does not perform as well as the last two approaches that fully leverage the label distribution. An enhanced FER+ data set with multiple labels for each face image will also be shared with the research community.},
	language = {en},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {Barsoum, Emad and Zhang, Cha and Ferrer, Cristian Canton and Zhang, Zhengyou},
	month = sep,
	year = {2016},
	note = {arXiv:1608.01041 [cs]},
	file = {Barsoum et al. - 2016 - Training Deep Networks for Facial Expression Recog.pdf:/Users/kawka/Zotero/storage/RSER9PYA/Barsoum et al. - 2016 - Training Deep Networks for Facial Expression Recog.pdf:application/pdf},
}
@ARTICLE{6475933,
    author={Mavadati, S. Mohammad and Mahoor, Mohammad H. and Bartlett, Kevin and Trinh, Philip and Cohn, Jeffrey F.},
    journal={IEEE Transactions on Affective Computing}, 
    title={DISFA: A Spontaneous Facial Action Intensity Database}, 
    year={2013},
    volume={4},
    number={2},
    pages={151-160},
    doi={10.1109/T-AFFC.2013.4}}

@misc{tfeid,
  	author = {Chen, L.F. and Yen, Y.S.},
  	title = {Taiwanese Facial Expression Image Database},
 	publisher = {Brain Mapping Laboratory, Institute of Brain Science, National Yang-Ming University, Taipei},
  	year = {2007},
}