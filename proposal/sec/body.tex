\section{Introduction}
\label{sec:intro}

Facial emotion recognition (FER)~\cite{Ko18} is not only an interesting in our daily life, 
but also important in the realm of artificial intelligence and computer vision. 
In this short proposal, 
we aim to leverage several deep neural networks to analyze and interpret different human facial emotions.

The structure of this report is arranged as follows. 
% \Cref{sec:related} contains the related work of our research. 
In~\Cref{sec:approach}, 
we provide the datasets we used, 
the model architecture we implemented. 
The preliminary evaluation results of our models are given in~\Cref{sec:result}. 
In~\Cref{sec:optim} describes the optimization strategies we have already used and plan to investigate in the coming weeks. 
Finally,
an overview of our time schedule for the entire final project is given in \Cref{fig:schedule}. 
Our code and supplementary material are available at \url{https://github.com/werywjw/SEP-CVDL}.

\begin{figure*}
  \resizebox{\textwidth}{!}{
    \begin{ganttchart}[
      % hgrid,
      vgrid,
      time slot format=isodate,
      x unit=0.3cm, 
      y unit title=0.9cm,
      y unit chart=0.45cm,
      bar height=0.35,
      bar top shift=0.2,
      group right shift=0,
      group top shift=0.1,
      group height=.3,
      group peaks width={0.2},
      milestone left shift=0.5,
      milestone right shift=0.5,
      title label font=\bfseries\Large
  ]{2023-12-21}{2024-02-23}
      \gantttitlecalendar{year, month} \\
      \ganttbar[bar/.append style={fill=LMUGreen}]{Data Collecting \& Processing}{2023-12-21}{2024-01-10} \\
      \ganttbar[bar/.append style={fill=LMUGreen}]{Model Implementing}{2023-12-30}{2024-01-30} \\
      \ganttbar[bar/.append style={fill=LMUGreen}]{Model Training \& Validating}{2023-12-30}{2024-01-30} \\
      \ganttbar[bar/.append style={fill=LMUGreen}]{Report Writing}{2024-01-04}{2024-01-16} \\
      \ganttgroup[bar/.append style={fill=LMUGreen}]{Preliminary Report}{2023-12-21}{2024-01-17} \\
      % \ganttmilestone{Deadline 1}{2024-01-18} \\
      \ganttbar[bar/.append style={fill=Orange}]{Explainable AI}{2024-01-05}{2024-01-30} \\
      \ganttbar[bar/.append style={fill=Orange}]{Model Fine-Tuning}{2024-01-05}{2024-01-30} \\
      \ganttbar[bar/.append style={fill=Orange}]{Video Preparing}{2024-01-17}{2024-02-06} \\
      \ganttbar[bar/.append style={fill=Orange}]{Slides Preparing}{2024-01-20}{2024-02-06} \\
      \ganttgroup[bar/.append style={fill=Orange}]{Presentation}{2024-01-05}{2024-02-07} \\
      % \ganttmilestone{Deadline 2}{2024-02-08} \\
      \ganttbar[bar/.append style={fill=cvprblue}]{Report Writing}{2024-01-17}{2024-02-21} \\
      \ganttgroup[bar/.append style={fill=cvprblue}]{Final Submission}{2024-01-19}{2024-02-22} \\
      % \ganttmilestone{Deadline 3}{2024-02-23}
      % \ganttlink{elem3}{elem8}
      % \ganttlink{elem2}{elem3}
      % \ganttlink{elem2}{elem4}
      % \ganttlink{elem4}{elem5}
  \end{ganttchart}
  }
  \caption{Overview of the schedule for the final project}
  \label{fig:schedule}
\end{figure*}

%-------------------------------------------------------------------------
% All authors will benefit from reading Mermin's description of how to write mathematics:
% \url{http://www.pamitc.org/documents/mermin.pdf}.


% \begin{figure*}
%   \centering
%   \begin{subfigure}{0.68\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{An example of a subfigure.}
%     \label{fig:short-a}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.28\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{Another example of a subfigure.}
%     \label{fig:short-b}
%   \end{subfigure}
%   \caption{Example of a short caption, which should be centered.}
%   \label{fig:short}
% \end{figure*}

\section{Approach}
\label{sec:approach}

\begin{figure}[ht]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  % \resizebox{.45\textwidth}{!}{
   \includegraphics[width=\linewidth]{output.png}
  % }
   \caption{Empirical results in terms of the loss and accuracy on different training epochs}
   \label{fig:result}
\end{figure}

\subsection{Dataset Acquisition and Processing}
To initiate the training, 
we acquired the databases FER+, RAF-DB, CK+ and TFEID from public institutions and GitHub-repositories.
Based on these databases we created a dataset by augmentation to increase variety, 
full details of augmentation is given in \Cref{sec:optim:aug}.
Firstly, 
for all the image data from the training dataset RAF-DB\footnote{\url{http://www.whdeng.cn/raf/model1.html}}~\cite{li2017reliable,li2019reliable}, 
we filter out neutral instances from the original dataset, 
the emotion labels are denoted as 1 (Surprised), 2 (Fearful), 3 (Disgusted), 
4 (Happy), 5 (Sad), and 6 (Angry) for simplicity 
(Our first dataset is downloaded from \url{https://www.kaggle.com/datasets/shuvoalok/raf-db-dataset/code} with the addition CSV file to their labels). 
The test result in \Cref{fig:result} is also aggregated from this specific dataset.
To transform and resize the images to \texttt{(64,64)}, 
we convert the images to greyscale with three channels as our original convolutional neural network (CNN) is designed to work with three-channel inputs. 
Also, we randomly flip the images horizontally with a default 50\% chance. 
This kind of augmentation helps in making the model more robust to orientation changes and thus improves the generalization ability. 
Our training dataset is aggregated from FER+~\cite{BarsoumZCZ16}, CK+~\cite{LuceyCKSAM10}.

\subsection{Model Architecture}
We implemented from scratch an emotion classification model with four convolution layers at the very beginning. 
Following each convolutional layer, 
batch normalization is applied. 
This stabilizes learning by normalizing the input to each layer. 
Then three linear layers are applied to extract features to the final output. 
We also add a \texttt{dropout} layer to prevent overfitting. 
The activation function used after each layer is Rectified Linear Unit (ReLU), 
since it introduces the non-linearity into the model, 
allowing it to learn more complex patterns. 
In order to find the best hyperparameter configuration (see \Cref{tab:hyper} for details) of the model, 
we utilize the parameter grid from sklearn~\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html}}. 

% model comparison, e.g., SVM try, also VGG
% specific dataset comparison, e.g., RAF-DB & FER+
% bold the best results later

\section{Preliminary Results}
\label{sec:result}

For evaluation, we use the metric accuracy. 
As seen in \Cref{tab:model}, 
we report all the training, testing, and validation accuracy in \% to improve the performance of our models. 
The loss function employed for all models is cross-entropy, which is typically for multi-class classification. 
The best results of the performance of the model with respect to loss and accuracy are depicted in \Cref{fig:result}.

\begin{table}
  \centering
  \resizebox{.47\textwidth}{!}{
  \begin{tabular}{@{}lccc@{}}
    \toprule
    Models &  Accuracy (Train) &  Accuracy (Test) & Accuracy (Vali) \\
    \midrule
    CNN (Baseline) & 66.3 & 75.2 & 52.6 \\
    CNN (SE) & 74.3 & 79.9 & 59.6 \\
    CNN (SE-Aug) & 84.6 & 79.5 & 62.8 \\
    CNN (SE+Residual) & 71.5 & 78.9 & 56.4 \\
    ResNet18~\cite{HeZRS16} & 76.8 & 79.8 & 60.3 \\
    \bottomrule
  \end{tabular}
  }
  \caption{Accuracy (\%) for different models in our experiments}
  \label{tab:model}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{@{}lc@{}}
      \toprule
      Hyperparameter & Configuration \\
      \midrule
      Learning rate & \{0.1, 0.01, 0.001, 0.0001\}  \\
      Batch size & \{8, 16, 32, 64\} \\
      Dropout rate & \{0.5\} \\
      Epoch & \{10, 20, 30, 40\} \\
      Early stopping & \{\texttt{True}, \texttt{False}\} \\
      Patience & \{5\} \\
      \bottomrule
    \end{tabular}
    \caption{Explored hyperparameter space for our models}
    \label{tab:hyper}
\end{table}

\section{Optimization Strategies}
\label{sec:optim}
We increase the depth of the network by adding some convolutional layers to learn more complex features. 
We also add the residual connections to help the training of deeper networks more efficiently, 
as they allow gradients to flow through the network more easily, improving the training for deep architectures. 
Moreover, 
we add squeeze and excitation (SE) blocks to apply channel-wise attention. 
In the coming weeks, 
we will focus on the following the tasks~\Cref{sec:optim:aug} and ~\Cref{sec:optim:video}.

\subsection{Augmentation}
\label{sec:optim:aug}
In the realm of machine learning and artificial intelligence, 
augmentation stands as a transformative technique, 
empowering algorithms to learn from and adapt to a wider range of data. 
By introducing subtle modifications to existing data points, 
augmentation effectively expands the dataset, 
enabling models to generalize better and achieve enhanced performance.
As models encounter slightly altered versions of familiar data, 
they are forced to make more nuanced and robust predictions. 
With this process we aim to prevent overfitting, 
a common pitfall in machine learning. 
Additionally we guide the training process to enhance recognition and handling of real-world variations.
During the project we pursue various approaches. 
We are implementing different combinations of functions from the \texttt{pytorch.transforms} library and testing already established filters that have been developed, in other research contexts. 
We create various replications of existing photos by randomly altering different properties such as size, brightness, color channels, or perspectives.

\subsection{Video Green-Square}
\label{sec:optim:video}
A colored-frame on a video or image is a visualization to highlight the regions that contributed most to the model's prediction. 
In this validation interaction, 
the colored frame, marks the pixels that are used to identify the depicted emotion. 
In addition to a colored frame, 
for example a green square, 
the written percentage specification of the current emotions should be displayed. 
This task will be implemented by using the library OpenCV~\footnote{~\url{https://opencv.org}}.

\subsection*{Author Contributions}
\label{sec:author}
% see https://arxiv.org/pdf/2005.14165.pdf page42
Equal contributions are listed by alphabetical order of surnames. 
Every author did the literature research and contributed to the writing of the paper. 

\begin{itemize}
  \item \textbf{Tanja Jaschkowitz} implemented the model architecture, training and testing infrastructure, and CSV file aggregations. 
  \item \textbf{Leah Kawka} collected the training data, prepared data processing, implemented augmentation, and ran the results. 
  She also takes part in the explainable AI, specificly in implementing the video-green squares.
  \item \textbf{Mahdi Mohammadi} implemented the augmentation, and did the research searching. 
  \item \textbf{Jiawen Wang} implemented the model architecture, training and testing infrastructure, and optimization strategies. 
  In the specific writing part, she also checked and aggeregated this report from other team members.
\end{itemize}

\section*{Acknowledgements}

We are deeply grateful to our advisors \textbf{Johannes Fischer} and \textbf{Ming Gui} for their helpful and valuable support during the entire semester. 
We also thank \textbf{Prof. Dr. Bj√∂rn Ommer} for providing this interesting practical course.